{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe007f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai             # For LLM interaction\n",
    "import json               # For parsing LLM responses\n",
    "import networkx as nx     # For creating and managing the graph data structure\n",
    "import ipycytoscape       # For interactive in-notebook graph visualization\n",
    "import ipywidgets         # For interactive elements\n",
    "import pandas as pd       # For displaying data in tables\n",
    "import os                 # For accessing environment variables (safer for API keys)\n",
    "import math               # For basic math operations\n",
    "import re                 # For basic text cleaning (regular expressions)\n",
    "import warnings           # To suppress potential deprecation warnings\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84bba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructured_text = \"\"\"\n",
    "Marie Curie, born Maria Skłodowska in Warsaw, Poland, was a pioneering physicist and chemist.\n",
    "She conducted groundbreaking research on radioactivity. Together with her husband, Pierre Curie,\n",
    "she discovered the elements polonium and radium. Marie Curie was the first woman to win a Nobel Prize,\n",
    "the first person and only woman to win the Nobel Prize twice, and the only person to win the Nobel Prize\n",
    "in two different scientific fields. She won the Nobel Prize in Physics in 1903 with Pierre Curie\n",
    "and Henri Becquerel. Later, she won the Nobel Prize in Chemistry in 1911 for her work on radium and\n",
    "polonium. During World War I, she developed mobile radiography units, known as 'petites Curies',\n",
    "to provide X-ray services to field hospitals. Marie Curie died in 1934 from aplastic anemia, likely\n",
    "caused by her long-term exposure to radiation.\n",
    "\n",
    "Marie was born on November 7, 1867, to a family of teachers who valued education. She received her\n",
    "early schooling in Warsaw but moved to Paris in 1891 to continue her studies at the Sorbonne, where\n",
    "she earned degrees in physics and mathematics. She met Pierre Curie, a professor of physics, in 1894, \n",
    "and they married in 1895, beginning a productive scientific partnership. Following Pierre's tragic \n",
    "death in a street accident in 1906, Marie took over his teaching position, becoming the first female \n",
    "professor at the Sorbonne.\n",
    "\n",
    "The Curies' work on radioactivity was conducted in challenging conditions, in a poorly equipped shed \n",
    "with no proper ventilation, as they processed tons of pitchblende ore to isolate radium. Marie Curie\n",
    "established the Curie Institute in Paris, which became a major center for medical research. She had\n",
    "two daughters: Irène, who later won a Nobel Prize in Chemistry with her husband, and Eve, who became\n",
    "a writer. Marie's notebooks are still radioactive today and are kept in lead-lined boxes. Her legacy\n",
    "includes not only her scientific discoveries but also her role in breaking gender barriers in academia\n",
    "and science.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bdabee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input Text Loaded ---\n",
      "\n",
      "Marie Curie, born Maria Skłodowska in Warsaw, Poland, was a pioneering physicist and chemist.\n",
      "She conducted groundbreaking research on radioactivity. Together with her husband, Pierre Curie,\n",
      "she discovered the elements polonium and radium. Marie Curie was the first woman to win a Nobel Prize,\n",
      "the first person and only woman to win the Nobel Prize twice, and the only person to win the Nobel Prize\n",
      "in two different scientific fields. She won the Nobel Prize in Physics in 1903 with Pierre Curie\n",
      "and Henri Becquerel. Later, she won the Nobel Prize in Chemistry in 1911 for her work on radium and\n",
      "polonium. During World War I, she developed mobile radiography units, known as 'petites Curies',\n",
      "to provide X-ray services to field hospitals. Marie Curie died in 1934 from aplastic anemia, likely\n",
      "caused by her long-term exposure to radiation.\n",
      "\n",
      "Marie was born on November 7, 1867, to a family of teachers who valued education. She received her\n",
      "early schooling in Warsaw but moved to Paris in 1891 to continue her studies at the Sorbonne, where\n",
      "she earned degrees in physics and mathematics. She met Pierre Curie, a professor of physics, in 1894, \n",
      "and they married in 1895, beginning a productive scientific partnership. Following Pierre's tragic \n",
      "death in a street accident in 1906, Marie took over his teaching position, becoming the first female \n",
      "professor at the Sorbonne.\n",
      "\n",
      "The Curies' work on radioactivity was conducted in challenging conditions, in a poorly equipped shed \n",
      "with no proper ventilation, as they processed tons of pitchblende ore to isolate radium. Marie Curie\n",
      "established the Curie Institute in Paris, which became a major center for medical research. She had\n",
      "two daughters: Irène, who later won a Nobel Prize in Chemistry with her husband, and Eve, who became\n",
      "a writer. Marie's notebooks are still radioactive today and are kept in lead-lined boxes. Her legacy\n",
      "includes not only her scientific discoveries but also her role in breaking gender barriers in academia\n",
      "and science.\n",
      "\n",
      "-------------------------\n",
      "Total characters: 1995\n",
      "Approximate word count: 324\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Input Text Loaded ---\")\n",
    "print(unstructured_text)\n",
    "print(\"-\" * 25)\n",
    "# Basic stats visualization\n",
    "char_count = len(unstructured_text)\n",
    "word_count = len(unstructured_text.split())\n",
    "print(f\"Total characters: {char_count}\")\n",
    "print(f\"Approximate word count: {word_count}\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f776329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size set to: 150 words\n",
      "Overlap set to: 30 words\n",
      "Chunking configuration is valid.\n"
     ]
    }
   ],
   "source": [
    "# --- Chunking Configuration ---\n",
    "chunk_size = 150  # Number of words per chunk (adjust as needed)\n",
    "overlap = 30     # Number of words to overlap (must be < chunk_size)\n",
    "\n",
    "print(f\"Chunk Size set to: {chunk_size} words\")\n",
    "print(f\"Overlap set to: {overlap} words\")\n",
    "\n",
    "# --- Basic Validation ---\n",
    "if overlap >= chunk_size and chunk_size > 0:\n",
    "    print(f\"Error: Overlap ({overlap}) must be smaller than chunk size ({chunk_size}).\")\n",
    "    raise SystemExit(\"Chunking configuration error.\")\n",
    "else:\n",
    "    print(\"Chunking configuration is valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9305cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 324 words.\n",
      "First 20 words: ['Marie', 'Curie,', 'born', 'Maria', 'Skłodowska', 'in', 'Warsaw,', 'Poland,', 'was', 'a', 'pioneering', 'physicist', 'and', 'chemist.', 'She', 'conducted', 'groundbreaking', 'research', 'on', 'radioactivity.']\n"
     ]
    }
   ],
   "source": [
    "words = unstructured_text.split()\n",
    "total_words = len(words)\n",
    "\n",
    "print(f\"Text split into {total_words} words.\")\n",
    "# Visualize the first 20 words\n",
    "print(f\"First 20 words: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe9765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chunking process...\n",
      "\n",
      "Text successfully split into 3 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "start_index = 0\n",
    "chunk_number = 1\n",
    "\n",
    "print(f\"Starting chunking process...\")\n",
    "\n",
    "while start_index < total_words:\n",
    "    end_index = min(start_index + chunk_size, total_words)\n",
    "    chunk_text = \" \".join(words[start_index:end_index])\n",
    "    chunks.append({\"text\": chunk_text, \"chunk_number\": chunk_number})\n",
    "\n",
    "    # print(f\"  Created chunk {chunk_number}: words {start_index} to {end_index-1}\") # Uncomment for detailed log\n",
    "\n",
    "    # Calculate the start of the next chunk\n",
    "    next_start_index = start_index + chunk_size - overlap\n",
    "\n",
    "    # Ensure progress is made\n",
    "    if next_start_index <= start_index:\n",
    "        if end_index == total_words:\n",
    "             break # Already processed the last part\n",
    "        next_start_index = start_index + 1\n",
    "\n",
    "    start_index = next_start_index\n",
    "    chunk_number += 1\n",
    "\n",
    "    # Safety break (optional)\n",
    "    if chunk_number > total_words: # Simple safety\n",
    "        print(\"Warning: Chunking loop exceeded total word count, breaking.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nText successfully split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42faad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk Details ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>word_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>Marie Curie, born Maria Skłodowska in Warsaw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>field hospitals. Marie Curie died in 1934 from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>with no proper ventilation, as they processed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_number  word_count                                               text\n",
       "0             1         150  Marie Curie, born Maria Skłodowska in Warsaw, ...\n",
       "1             2         150  field hospitals. Marie Curie died in 1934 from...\n",
       "2             3          84  with no proper ventilation, as they processed ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Chunk Details ---\")\n",
    "if chunks:\n",
    "    # Create a DataFrame for better visualization\n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    chunks_df['word_count'] = chunks_df['text'].apply(lambda x: len(x.split()))\n",
    "    display(chunks_df[['chunk_number', 'word_count', 'text']])\n",
    "else:\n",
    "    print(\"No chunks were created (text might be shorter than chunk size).\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4885fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   - **Text Chunk:** Marie Curie discovered Radium in 1898.\n",
    "\n",
    "#   - **System Prompt:** You are an expert in information extraction.\n",
    "\n",
    "#   - **User Prompt:** Extract SPO triples. Rules:\n",
    "#   - Follow the pattern.\n",
    "#   - Text: text__chunk_placeholder.\n",
    "#   - Required JSON format: Your JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27efb722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI API Key: ***HnEA\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"Using OpenAI API Key: {'***' + api_key[-4:] if api_key else 'Not Set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40045eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System Prompt: Sets the context/role for the LLM ---\n",
    "extraction_system_prompt = \"\"\"\n",
    "You are an AI expert specialized in knowledge graph extraction.\n",
    "Your task is to identify and extract factual Subject-Predicate-Object (SPO) triples from the given text.\n",
    "Focus on accuracy and adhere strictly to the JSON output format requested in the user prompt.\n",
    "Extract core entities and the most direct relationship.\n",
    "\"\"\"\n",
    "\n",
    "# --- User Prompt Template: Contains specific instructions and the text ---\n",
    "extraction_user_prompt_template = \"\"\"\n",
    "Please extract Subject-Predicate-Object (S-P-O) triples from the text below.\n",
    "\n",
    "**VERY IMPORTANT RULES:**\n",
    "1.  **Output Format:** Respond ONLY with a single, valid JSON array. Each element MUST be an object with keys \"subject\", \"predicate\", \"object\".\n",
    "2.  **JSON Only:** Do NOT include any text before or after the JSON array (e.g., no 'Here is the JSON:' or explanations). Do NOT use markdown ```json ... ``` tags.\n",
    "3.  **Concise Predicates:** Keep the 'predicate' value concise (1-3 words, ideally 1-2). Use verbs or short verb phrases (e.g., 'discovered', 'was born in', 'won').\n",
    "4.  **Lowercase:** ALL values for 'subject', 'predicate', and 'object' MUST be lowercase.\n",
    "5.  **Pronoun Resolution:** Replace pronouns (she, he, it, her, etc.) with the specific lowercase entity name they refer to based on the text context (e.g., 'marie curie').\n",
    "6.  **Specificity:** Capture specific details (e.g., 'nobel prize in physics' instead of just 'nobel prize' if specified).\n",
    "7.  **Completeness:** Extract all distinct factual relationships mentioned.\n",
    "\n",
    "**Text to Process:**\n",
    "{text_chunk}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b567c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Chunks\n",
    "#       |\n",
    "# Format Prompt\n",
    "# System + User + Chunk\n",
    "#       |\n",
    "# Send to LLM API\n",
    "#       |\n",
    "# Receive Response\n",
    "#       |\n",
    "# Parse JSON\n",
    "#    /      \\\n",
    "# Validate Triples   if failure\n",
    "#    /     \\        \n",
    "# if invalid    Handle Errors / Failures\n",
    "#    \\     /\n",
    "# Store Valid Triples\n",
    "#       |\n",
    "#     (loop back to Text Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5045388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting triple extraction from 3 chunks using model 'gpt-3.5-turbo'...\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results and failures\n",
    "all_extracted_triples = []\n",
    "failed_chunks = []\n",
    "\n",
    "chunk_index = 0  # Process first chunk only\n",
    "\n",
    "# --- Define LLM Call Parameters ---\n",
    "# Choose the model available at your configured endpoint.\n",
    "# Examples: 'gpt-4o', 'gpt-3.5-turbo', 'llama3', 'mistral', 'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', 'gemma'\n",
    "\n",
    "# llm_model_name = \"google/flan-t5-small\"\n",
    "llm_model_name = \"gpt-3.5-turbo\"\n",
    "# llm_model_name=\"deepseek-ai/DeepSeek-V3\" # Example for a specific model\n",
    "# llm_max_tokens = 256\n",
    "max_input_tokens = 512\n",
    "\n",
    "llm_temperature = 0.0 # Lower temperature for more deterministic, factual output. 0.0 is best for extraction.\n",
    "llm_max_tokens = 4096 # Max tokens for the LLM response (adjust based on model limits)\n",
    "\n",
    "print(f\"Starting triple extraction from {len(chunks)} chunks using model '{llm_model_name}'...\")\n",
    "# We will process chunks one by one in the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323b023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\n",
    "# #     llm_model_name,\n",
    "# #     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # or float32 if no GPU or limited hardware\n",
    "# # )\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d56b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk length: 3\n"
     ]
    }
   ],
   "source": [
    "print (f\"chunk length: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df35b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 1/3 --- \n",
      "1. Formatting User Prompt...\n",
      "2. Sending request to LLM...\n",
      "   LLM response received.\n",
      "3. Extracting raw response content...\n",
      "--- Raw LLM Output (Chunk 1) ---\n",
      "{\n",
      "    \"subject\": \"marie curie\",\n",
      "    \"predicate\": \"was born in\",\n",
      "    \"object\": \"warsaw, poland\"\n",
      "  }\n",
      "--------------------\n",
      "4. Attempting to parse JSON from response...\n",
      "   Detected single SPO triple dict. Wrapping in list.\n",
      "--- Parsed JSON Data (Chunk 1) ---\n",
      "[\n",
      "  {\n",
      "    \"subject\": \"marie curie\",\n",
      "    \"predicate\": \"was born in\",\n",
      "    \"object\": \"warsaw, poland\"\n",
      "  }\n",
      "]\n",
      "--------------------\n",
      "5. Validating structure and extracting triples...\n",
      "   Found 1 valid triples in this chunk.\n",
      "--- Valid Triples Extracted (Chunk 1) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>was born in</td>\n",
       "      <td>warsaw, poland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    predicate          object  chunk\n",
       "0  marie curie  was born in  warsaw, poland      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--- Running Total Triples Extracted: 4 --- \n",
      "--- Failed Chunks So Far: 0 --- \n",
      "\n",
      "--- Processing Chunk 2/3 --- \n",
      "1. Formatting User Prompt...\n",
      "2. Sending request to LLM...\n",
      "   LLM response received.\n",
      "3. Extracting raw response content...\n",
      "--- Raw LLM Output (Chunk 2) ---\n",
      "{\n",
      "\t\t\"subject\": \"marie curie\",\n",
      "\t\t\"predicate\": \"died in\",\n",
      "\t\t\"object\": \"1934\"\n",
      "\t}\n",
      "--------------------\n",
      "4. Attempting to parse JSON from response...\n",
      "   Detected single SPO triple dict. Wrapping in list.\n",
      "--- Parsed JSON Data (Chunk 2) ---\n",
      "[\n",
      "  {\n",
      "    \"subject\": \"marie curie\",\n",
      "    \"predicate\": \"died in\",\n",
      "    \"object\": \"1934\"\n",
      "  }\n",
      "]\n",
      "--------------------\n",
      "5. Validating structure and extracting triples...\n",
      "   Found 1 valid triples in this chunk.\n",
      "--- Valid Triples Extracted (Chunk 2) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>died in</td>\n",
       "      <td>1934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject predicate object  chunk\n",
       "0  marie curie   died in   1934      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--- Running Total Triples Extracted: 5 --- \n",
      "--- Failed Chunks So Far: 0 --- \n",
      "\n",
      "--- Processing Chunk 3/3 --- \n",
      "1. Formatting User Prompt...\n",
      "2. Sending request to LLM...\n",
      "   LLM response received.\n",
      "3. Extracting raw response content...\n",
      "--- Raw LLM Output (Chunk 3) ---\n",
      "{\n",
      "    \"subject\": \"marie curie\",\n",
      "    \"predicate\": \"established\",\n",
      "    \"object\": \"curie institute\"\n",
      "  }\n",
      "--------------------\n",
      "4. Attempting to parse JSON from response...\n",
      "   Detected single SPO triple dict. Wrapping in list.\n",
      "--- Parsed JSON Data (Chunk 3) ---\n",
      "[\n",
      "  {\n",
      "    \"subject\": \"marie curie\",\n",
      "    \"predicate\": \"established\",\n",
      "    \"object\": \"curie institute\"\n",
      "  }\n",
      "]\n",
      "--------------------\n",
      "5. Validating structure and extracting triples...\n",
      "   Found 1 valid triples in this chunk.\n",
      "--- Valid Triples Extracted (Chunk 3) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>established</td>\n",
       "      <td>curie institute</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    predicate           object  chunk\n",
       "0  marie curie  established  curie institute      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--- Running Total Triples Extracted: 6 --- \n",
      "--- Failed Chunks So Far: 0 --- \n"
     ]
    }
   ],
   "source": [
    "openai.api_key = api_key\n",
    "\n",
    "for chunk_index, chunk_info in enumerate(chunks):\n",
    "    chunk_text = chunk_info['text']\n",
    "    chunk_num = chunk_info['chunk_number']\n",
    "    \n",
    "    print(f\"\\n--- Processing Chunk {chunk_num}/{len(chunks)} --- \")\n",
    "    \n",
    "    # 1. Format the User Prompt\n",
    "    print(\"1. Formatting User Prompt...\")\n",
    "    user_prompt = extraction_user_prompt_template.format(text_chunk=chunk_text)\n",
    "    \n",
    "    llm_output = None\n",
    "    error_message = None\n",
    "    \n",
    "    try:\n",
    "        # 2. Make the API Call\n",
    "        print(\"2. Sending request to LLM...\")\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=llm_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": extraction_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=llm_temperature,\n",
    "            max_tokens=llm_max_tokens,\n",
    "            response_format={ \"type\": \"json_object\" }, \n",
    "        )\n",
    "        print(\"   LLM response received.\")\n",
    "        \n",
    "        # 3. Extract Raw Response Content\n",
    "        print(\"3. Extracting raw response content...\")\n",
    "        llm_output = response.choices[0].message.content.strip()\n",
    "        print(f\"--- Raw LLM Output (Chunk {chunk_num}) ---\")\n",
    "        print(llm_output)\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        # 4. Parse JSON (if API call succeeded)\n",
    "        parsed_json = None\n",
    "        parsing_error = None\n",
    "        if llm_output is not None:\n",
    "            print(\"4. Attempting to parse JSON from response...\")\n",
    "            try:\n",
    "                parsed_data = json.loads(llm_output)\n",
    "\n",
    "                if isinstance(parsed_data, list):\n",
    "                    parsed_json = parsed_data\n",
    "                    print(\"   Successfully parsed JSON list directly.\")\n",
    "\n",
    "                elif isinstance(parsed_data, dict):\n",
    "                    list_values = [v for v in parsed_data.values() if isinstance(v, list)]\n",
    "                    if len(list_values) == 1:\n",
    "                        parsed_json = list_values[0]\n",
    "                        print(\"   Extracted list from dictionary.\")\n",
    "                    else:\n",
    "                        print(\"   Detected single SPO triple dict. Wrapping in list.\")\n",
    "                        parsed_json = [parsed_data]\n",
    "                else:\n",
    "                    raise ValueError(\"Parsed JSON is not a list or a valid dict.\")\n",
    "\n",
    "            except json.JSONDecodeError as json_err:\n",
    "                parsing_error = f\"JSONDecodeError: {json_err}. Trying regex fallback...\"\n",
    "                print(f\"   {parsing_error}\")\n",
    "                match = re.search(r'^\\s*(\\[.*?\\])\\s*$', llm_output, re.DOTALL)\n",
    "                if match:\n",
    "                    json_string_extracted = match.group(1)\n",
    "                    print(\"      Regex found potential JSON array structure.\")\n",
    "                    try:\n",
    "                        parsed_json = json.loads(json_string_extracted)\n",
    "                        print(\"      Successfully parsed JSON from regex extraction.\")\n",
    "                        parsing_error = None\n",
    "                    except json.JSONDecodeError as nested_err:\n",
    "                        parsing_error = f\"JSONDecodeError after regex: {nested_err}\"\n",
    "                        print(f\"      ERROR: Regex content is not valid JSON: {nested_err}\")\n",
    "                else:\n",
    "                    parsing_error = \"JSONDecodeError and Regex fallback failed.\"\n",
    "                    print(\"      ERROR: Regex could not find JSON array structure.\")\n",
    "\n",
    "            except ValueError as val_err:\n",
    "                parsing_error = f\"ValueError: {val_err}\"\n",
    "                print(f\"   ERROR: {parsing_error}\")\n",
    "\n",
    "            if parsed_json is not None:\n",
    "                print(f\"--- Parsed JSON Data (Chunk {chunk_num}) ---\")\n",
    "                print(json.dumps(parsed_json, indent=2))\n",
    "                print(\"-\" * 20)\n",
    "            else:\n",
    "                print(f\"--- JSON Parsing FAILED (Chunk {chunk_num}) --- \")\n",
    "                print(f\"   Final Parsing Error: {parsing_error}\")\n",
    "                print(\"-\" * 20)\n",
    "                failed_chunks.append({'chunk_number': chunk_num, 'error': f'Parsing Failed: {parsing_error}', 'response': llm_output})\n",
    "\n",
    "        # 5. Validate and Store Triples (if parsing succeeded)\n",
    "        if parsed_json is not None:\n",
    "            print(\"5. Validating structure and extracting triples...\")\n",
    "            valid_triples_in_chunk = []\n",
    "            invalid_entries = []\n",
    "            if isinstance(parsed_json, list):\n",
    "                for item in parsed_json:\n",
    "                    if isinstance(item, dict) and all(k in item for k in ['subject', 'predicate', 'object']):\n",
    "                        if all(isinstance(item[k], str) for k in ['subject', 'predicate', 'object']):\n",
    "                            item['chunk'] = chunk_num\n",
    "                            valid_triples_in_chunk.append(item)\n",
    "                        else:\n",
    "                            invalid_entries.append({'item': item, 'reason': 'Non-string value'}) \n",
    "                    else:\n",
    "                        invalid_entries.append({'item': item, 'reason': 'Incorrect structure/keys'})\n",
    "            else:\n",
    "                print(\"   ERROR: Parsed data is not a list, cannot extract triples.\")\n",
    "                invalid_entries.append({'item': parsed_json, 'reason': 'Not a list'})\n",
    "                if not any(fc['chunk_number'] == chunk_num for fc in failed_chunks):\n",
    "                    failed_chunks.append({'chunk_number': chunk_num, 'error': 'Parsed data not a list', 'response': llm_output})\n",
    "            \n",
    "            print(f\"   Found {len(valid_triples_in_chunk)} valid triples in this chunk.\")\n",
    "            if invalid_entries:\n",
    "                print(f\"   Skipped {len(invalid_entries)} invalid entries.\")\n",
    "                \n",
    "            if valid_triples_in_chunk:\n",
    "                print(f\"--- Valid Triples Extracted (Chunk {chunk_num}) ---\")\n",
    "                display(pd.DataFrame(valid_triples_in_chunk))\n",
    "                print(\"-\" * 20)\n",
    "                all_extracted_triples.extend(valid_triples_in_chunk)\n",
    "            else:\n",
    "                print(f\"--- No valid triples extracted from this chunk. ---\")\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "        print(f\"--- Running Total Triples Extracted: {len(all_extracted_triples)} --- \")\n",
    "        print(f\"--- Failed Chunks So Far: {len(failed_chunks)} --- \")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"   ERROR during API call: {error_message}\")\n",
    "        failed_chunks.append({'chunk_number': chunk_num, 'error': f'API/Processing Error: {error_message}', 'response': ''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80351f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Extraction Summary ---\n",
      "Total chunks defined: 3\n",
      "Chunks processed (attempted): 3\n",
      "Total valid triples extracted across all processed chunks: 6\n",
      "Number of chunks that failed API call or parsing: 0\n",
      "-------------------------\n",
      "\n",
      "--- All Extracted Triples (Before Normalization) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>was born in</td>\n",
       "      <td>warsaw, poland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>died in</td>\n",
       "      <td>1934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>established</td>\n",
       "      <td>curie institute</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>was born in</td>\n",
       "      <td>warsaw, poland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>died in</td>\n",
       "      <td>1934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>established</td>\n",
       "      <td>curie institute</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    predicate           object  chunk\n",
       "0  marie curie  was born in   warsaw, poland      1\n",
       "1  marie curie      died in             1934      2\n",
       "2  marie curie  established  curie institute      3\n",
       "3  marie curie  was born in   warsaw, poland      1\n",
       "4  marie curie      died in             1934      2\n",
       "5  marie curie  established  curie institute      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Summary of Extraction (Reflecting state after the single chunk demo) ---\n",
    "print(f\"\\n--- Overall Extraction Summary ---\")\n",
    "print(f\"Total chunks defined: {len(chunks)}\")\n",
    "processed_chunks = len(chunks) - len(failed_chunks) # Approximation if loop isn't run fully\n",
    "print(f\"Chunks processed (attempted): {processed_chunks + len(failed_chunks)}\") # Chunks we looped through\n",
    "print(f\"Total valid triples extracted across all processed chunks: {len(all_extracted_triples)}\")\n",
    "print(f\"Number of chunks that failed API call or parsing: {len(failed_chunks)}\")\n",
    "\n",
    "if failed_chunks:\n",
    "    print(\"\\nDetails of Failed Chunks:\")\n",
    "    for failure in failed_chunks:\n",
    "        print(f\"  Chunk {failure['chunk_number']}: Error: {failure['error']}\")\n",
    "        # print(f\"    Response (start): {failure.get('response', '')[:100]}...\") # Uncomment for more detail\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Display all extracted triples using Pandas\n",
    "print(\"\\n--- All Extracted Triples (Before Normalization) ---\")\n",
    "if all_extracted_triples:\n",
    "    all_triples_df = pd.DataFrame(all_extracted_triples)\n",
    "    display(all_triples_df)\n",
    "else:\n",
    "    print(\"No triples were successfully extracted.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9f6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting normalization and de-duplication of 6 triples...\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists and tracking variables\n",
    "\n",
    "# Normalize: Trim whitespace, convert to lowercase.\n",
    "# Filter: Remove triples with empty parts after normalization.\n",
    "# De-duplicate: Remove exact duplicate (subject, predicate, object) combinations.\n",
    "\n",
    "normalized_triples = []\n",
    "seen_triples = set() # Tracks (subject, predicate, object) tuples\n",
    "original_count = len(all_extracted_triples)\n",
    "empty_removed_count = 0\n",
    "duplicates_removed_count = 0\n",
    "\n",
    "print(f\"Starting normalization and de-duplication of {original_count} triples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab5682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing triples for normalization (showing first 5 examples):\n",
      "\n",
      "--- Example 1 ---\n",
      "Original Triple (Chunk 1): {'subject': 'marie curie', 'predicate': 'was born in', 'object': 'warsaw, poland', 'chunk': 1}\n",
      "Normalized: SUB='marie curie', PRED='was born in', OBJ='warsaw, poland'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 2 ---\n",
      "Original Triple (Chunk 2): {'subject': 'marie curie', 'predicate': 'died in', 'object': '1934', 'chunk': 2}\n",
      "Normalized: SUB='marie curie', PRED='died in', OBJ='1934'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 3 ---\n",
      "Original Triple (Chunk 3): {'subject': 'marie curie', 'predicate': 'established', 'object': 'curie institute', 'chunk': 3}\n",
      "Normalized: SUB='marie curie', PRED='established', OBJ='curie institute'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 4 ---\n",
      "Original Triple (Chunk 1): {'subject': 'marie curie', 'predicate': 'was born in', 'object': 'warsaw, poland', 'chunk': 1}\n",
      "Normalized: SUB='marie curie', PRED='was born in', OBJ='warsaw, poland'\n",
      "Status: Discarded (Duplicate)\n",
      "\n",
      "--- Example 5 ---\n",
      "Original Triple (Chunk 2): {'subject': 'marie curie', 'predicate': 'died in', 'object': '1934', 'chunk': 2}\n",
      "Normalized: SUB='marie curie', PRED='died in', OBJ='1934'\n",
      "Status: Discarded (Duplicate)\n",
      "\n",
      "... Finished processing 6 triples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing triples for normalization (showing first 5 examples):\")\n",
    "example_limit = 5\n",
    "processed_count = 0\n",
    "\n",
    "for i, triple in enumerate(all_extracted_triples):\n",
    "    show_example = (i < example_limit)\n",
    "    if show_example:\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Original Triple (Chunk {triple.get('chunk', '?')}): {triple}\")\n",
    "        \n",
    "    subject_raw = triple.get('subject')\n",
    "    predicate_raw = triple.get('predicate')\n",
    "    object_raw = triple.get('object')\n",
    "    chunk_num = triple.get('chunk', 'unknown')\n",
    "    \n",
    "    triple_valid = False\n",
    "    normalized_sub, normalized_pred, normalized_obj = None, None, None\n",
    "\n",
    "    if isinstance(subject_raw, str) and isinstance(predicate_raw, str) and isinstance(object_raw, str):\n",
    "        # 1. Normalize\n",
    "        normalized_sub = subject_raw.strip().lower()\n",
    "        normalized_pred = re.sub(r'\\s+', ' ', predicate_raw.strip().lower()).strip()\n",
    "        normalized_obj = object_raw.strip().lower()\n",
    "        if show_example:\n",
    "            print(f\"Normalized: SUB='{normalized_sub}', PRED='{normalized_pred}', OBJ='{normalized_obj}'\")\n",
    "\n",
    "        # 2. Filter Empty\n",
    "        if normalized_sub and normalized_pred and normalized_obj:\n",
    "            triple_identifier = (normalized_sub, normalized_pred, normalized_obj)\n",
    "            \n",
    "            # 3. De-duplicate\n",
    "            if triple_identifier not in seen_triples:\n",
    "                normalized_triples.append({\n",
    "                    'subject': normalized_sub,\n",
    "                    'predicate': normalized_pred,\n",
    "                    'object': normalized_obj,\n",
    "                    'source_chunk': chunk_num\n",
    "                })\n",
    "                seen_triples.add(triple_identifier)\n",
    "                triple_valid = True\n",
    "                if show_example:\n",
    "                    print(\"Status: Kept (New Unique Triple)\")\n",
    "            else:\n",
    "                duplicates_removed_count += 1\n",
    "                if show_example:\n",
    "                    print(\"Status: Discarded (Duplicate)\")\n",
    "        else:\n",
    "            empty_removed_count += 1\n",
    "            if show_example:\n",
    "                print(\"Status: Discarded (Empty component after normalization)\")\n",
    "    else:\n",
    "        empty_removed_count += 1 # Count non-string/missing as needing removal\n",
    "        if show_example:\n",
    "             print(\"Status: Discarded (Non-string or missing component)\")\n",
    "    processed_count += 1\n",
    "\n",
    "print(f\"\\n... Finished processing {processed_count} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19ca7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalization & De-duplication Summary ---\n",
      "Original extracted triple count: 6\n",
      "Triples removed (empty/invalid components): 0\n",
      "Duplicate triples removed: 3\n",
      "Final unique, normalized triple count: 3\n",
      "-------------------------\n",
      "\n",
      "--- Final Normalized Triples ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>source_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>was born in</td>\n",
       "      <td>warsaw, poland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>died in</td>\n",
       "      <td>1934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>established</td>\n",
       "      <td>curie institute</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    predicate           object  source_chunk\n",
       "0  marie curie  was born in   warsaw, poland             1\n",
       "1  marie curie      died in             1934             2\n",
       "2  marie curie  established  curie institute             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Summary of Normalization --- \n",
    "print(f\"\\n--- Normalization & De-duplication Summary ---\")\n",
    "print(f\"Original extracted triple count: {original_count}\")\n",
    "print(f\"Triples removed (empty/invalid components): {empty_removed_count}\")\n",
    "print(f\"Duplicate triples removed: {duplicates_removed_count}\")\n",
    "final_count = len(normalized_triples)\n",
    "print(f\"Final unique, normalized triple count: {final_count}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Display a sample of normalized triples using Pandas\n",
    "print(\"\\n--- Final Normalized Triples ---\")\n",
    "if normalized_triples:\n",
    "    normalized_df = pd.DataFrame(normalized_triples)\n",
    "    display(normalized_df)\n",
    "else:\n",
    "    print(\"No valid triples remain after normalization.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c612068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty NetworkX DiGraph.\n",
      "--- Initial Graph Info ---\n",
      "Type: DiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create an empty directed graph\n",
    "knowledge_graph = nx.DiGraph()\n",
    "\n",
    "print(\"Initialized an empty NetworkX DiGraph.\")\n",
    "# Visualize the initial empty graph state\n",
    "print(\"--- Initial Graph Info ---\")\n",
    "try:\n",
    "    # Try the newer method first\n",
    "    print(nx.info(knowledge_graph))\n",
    "except AttributeError:\n",
    "    # Fallback for different NetworkX versions\n",
    "    print(f\"Type: {type(knowledge_graph).__name__}\")\n",
    "    print(f\"Number of nodes: {knowledge_graph.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {knowledge_graph.number_of_edges()}\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b85089a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding triples to the NetworkX graph...\n",
      "\n",
      "--- Graph Info after adding Triple #3 --- (marie curie -> curie institute)\n",
      "Type: DiGraph\n",
      "Number of nodes: 4\n",
      "Number of edges: 3\n",
      "\n",
      "Finished adding triples. Processed 3 edges.\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding triples to the NetworkX graph...\")\n",
    "\n",
    "added_edges_count = 0\n",
    "update_interval = 5 # How often to print graph info update\n",
    "\n",
    "if not normalized_triples:\n",
    "    print(\"Warning: No normalized triples to add to the graph.\")\n",
    "else:\n",
    "    for i, triple in enumerate(normalized_triples):\n",
    "        subject_node = triple['subject']\n",
    "        object_node = triple['object']\n",
    "        predicate_label = triple['predicate']\n",
    "        \n",
    "        # Nodes are added automatically when adding edges, but explicit calls are fine too\n",
    "        # knowledge_graph.add_node(subject_node) \n",
    "        # knowledge_graph.add_node(object_node)\n",
    "        \n",
    "        # Add the directed edge with the predicate as a 'label' attribute\n",
    "        knowledge_graph.add_edge(subject_node, object_node, label=predicate_label)\n",
    "        added_edges_count += 1\n",
    "        \n",
    "        # --- Visualize Graph Growth --- \n",
    "        if (i + 1) % update_interval == 0 or (i + 1) == len(normalized_triples):\n",
    "            print(f\"\\n--- Graph Info after adding Triple #{i+1} --- ({subject_node} -> {object_node})\")\n",
    "            try:\n",
    "                # Try the newer method first\n",
    "                print(nx.info(knowledge_graph))\n",
    "            except AttributeError:\n",
    "                # Fallback for different NetworkX versions\n",
    "                print(f\"Type: {type(knowledge_graph).__name__}\")\n",
    "                print(f\"Number of nodes: {knowledge_graph.number_of_nodes()}\")\n",
    "                print(f\"Number of edges: {knowledge_graph.number_of_edges()}\")\n",
    "            # For very large graphs, printing info too often can be slow. Adjust interval.\n",
    "\n",
    "print(f\"\\nFinished adding triples. Processed {added_edges_count} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b335dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final NetworkX Graph Summary ---\n",
      "Total unique nodes (entities): 4\n",
      "Total unique edges (relationships): 3\n",
      "Graph density: 0.2500\n",
      "The graph is weakly connected (all nodes reachable ignoring direction).\n",
      "-------------------------\n",
      "\n",
      "--- Sample Nodes (First 10) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warsaw, poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>curie institute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Node Sample\n",
       "0      marie curie\n",
       "1   warsaw, poland\n",
       "2             1934\n",
       "3  curie institute"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Edges (First 10 with Labels) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>warsaw, poland</td>\n",
       "      <td>was born in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>1934</td>\n",
       "      <td>died in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marie curie</td>\n",
       "      <td>curie institute</td>\n",
       "      <td>established</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source           Target        Label\n",
       "0  marie curie   warsaw, poland  was born in\n",
       "1  marie curie             1934      died in\n",
       "2  marie curie  curie institute  established"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Final Graph Statistics --- \n",
    "num_nodes = knowledge_graph.number_of_nodes()\n",
    "num_edges = knowledge_graph.number_of_edges()\n",
    "\n",
    "print(f\"\\n--- Final NetworkX Graph Summary ---\")\n",
    "print(f\"Total unique nodes (entities): {num_nodes}\")\n",
    "print(f\"Total unique edges (relationships): {num_edges}\")\n",
    "\n",
    "if num_edges != added_edges_count and isinstance(knowledge_graph, nx.DiGraph):\n",
    "     print(f\"Note: Added {added_edges_count} edges, but graph has {num_edges}. DiGraph overwrites edges with same source/target. Use MultiDiGraph if multiple edges needed.\")\n",
    "\n",
    "if num_nodes > 0:\n",
    "    try:\n",
    "       density = nx.density(knowledge_graph)\n",
    "       print(f\"Graph density: {density:.4f}\")\n",
    "       if nx.is_weakly_connected(knowledge_graph):\n",
    "           print(\"The graph is weakly connected (all nodes reachable ignoring direction).\")\n",
    "       else:\n",
    "           num_components = nx.number_weakly_connected_components(knowledge_graph)\n",
    "           print(f\"The graph has {num_components} weakly connected components.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate some graph metrics: {e}\") # Handle potential errors on empty/small graphs\n",
    "else:\n",
    "    print(\"Graph is empty, cannot calculate metrics.\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- Sample Nodes --- \n",
    "print(\"\\n--- Sample Nodes (First 10) ---\")\n",
    "if num_nodes > 0:\n",
    "    nodes_sample = list(knowledge_graph.nodes())[:10]\n",
    "    display(pd.DataFrame(nodes_sample, columns=['Node Sample']))\n",
    "else:\n",
    "    print(\"Graph has no nodes.\")\n",
    "\n",
    "# --- Sample Edges --- \n",
    "print(\"\\n--- Sample Edges (First 10 with Labels) ---\")\n",
    "if num_edges > 0:\n",
    "    edges_sample = []\n",
    "    for u, v, data in list(knowledge_graph.edges(data=True))[:10]:\n",
    "        edges_sample.append({'Source': u, 'Target': v, 'Label': data.get('label', 'N/A')})\n",
    "    display(pd.DataFrame(edges_sample))\n",
    "else:\n",
    "    print(\"Graph has no edges.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b555a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing interactive visualization...\n",
      "Graph seems valid for visualization (4 nodes, 3 edges).\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing interactive visualization...\")\n",
    "\n",
    "# --- Check Graph Validity for Visualization --- \n",
    "can_visualize = False\n",
    "if 'knowledge_graph' not in locals() or not isinstance(knowledge_graph, nx.Graph):\n",
    "    print(\"Error: 'knowledge_graph' not found or is not a NetworkX graph.\")\n",
    "elif knowledge_graph.number_of_nodes() == 0:\n",
    "    print(\"NetworkX Graph is empty. Cannot visualize.\")\n",
    "else:\n",
    "    print(f\"Graph seems valid for visualization ({knowledge_graph.number_of_nodes()} nodes, {knowledge_graph.number_of_edges()} edges).\")\n",
    "    can_visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3744d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting nodes...\n",
      "Converted 4 nodes.\n",
      "Converting edges...\n",
      "Converted 3 edges.\n",
      "\n",
      "--- Sample Cytoscape Node Data (First 2) ---\n",
      "[\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"marie curie\",\n",
      "      \"label\": \"marie\\ncurie\",\n",
      "      \"degree\": 3,\n",
      "      \"size\": 65.0,\n",
      "      \"tooltip_text\": \"Entity: marie curie\\nDegree: 3\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"warsaw, poland\",\n",
      "      \"label\": \"warsaw,\\npoland\",\n",
      "      \"degree\": 1,\n",
      "      \"size\": 31.666666666666664,\n",
      "      \"tooltip_text\": \"Entity: warsaw, poland\\nDegree: 1\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Sample Cytoscape Edge Data (First 2) ---\n",
      "[\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"edge_0\",\n",
      "      \"source\": \"marie curie\",\n",
      "      \"target\": \"warsaw, poland\",\n",
      "      \"label\": \"was born in\",\n",
      "      \"tooltip_text\": \"Relationship: was born in\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"edge_1\",\n",
      "      \"source\": \"marie curie\",\n",
      "      \"target\": \"1934\",\n",
      "      \"label\": \"died in\",\n",
      "      \"tooltip_text\": \"Relationship: died in\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "cytoscape_nodes = []\n",
    "cytoscape_edges = []\n",
    "\n",
    "if can_visualize:\n",
    "    print(\"Converting nodes...\")\n",
    "    # Calculate degrees for node sizing\n",
    "    node_degrees = dict(knowledge_graph.degree())\n",
    "    max_degree = max(node_degrees.values()) if node_degrees else 1\n",
    "    \n",
    "    for node_id in knowledge_graph.nodes():\n",
    "        degree = node_degrees.get(node_id, 0)\n",
    "        # Simple scaling for node size (adjust logic as needed)\n",
    "        node_size = 15 + (degree / max_degree) * 50 if max_degree > 0 else 15\n",
    "        \n",
    "        cytoscape_nodes.append({\n",
    "            'data': {\n",
    "                'id': str(node_id), # ID must be string\n",
    "                'label': str(node_id).replace(' ', '\\n'), # Display label (wrap spaces)\n",
    "                'degree': degree,\n",
    "                'size': node_size,\n",
    "                'tooltip_text': f\"Entity: {str(node_id)}\\nDegree: {degree}\" # Tooltip on hover\n",
    "            }\n",
    "        })\n",
    "    print(f\"Converted {len(cytoscape_nodes)} nodes.\")\n",
    "    \n",
    "    print(\"Converting edges...\")\n",
    "    edge_count = 0\n",
    "    for u, v, data in knowledge_graph.edges(data=True):\n",
    "        edge_id = f\"edge_{edge_count}\" # Unique edge ID\n",
    "        predicate_label = data.get('label', '')\n",
    "        cytoscape_edges.append({\n",
    "            'data': {\n",
    "                'id': edge_id,\n",
    "                'source': str(u),\n",
    "                'target': str(v),\n",
    "                'label': predicate_label, # Label on edge\n",
    "                'tooltip_text': f\"Relationship: {predicate_label}\" # Tooltip on hover\n",
    "            }\n",
    "        })\n",
    "        edge_count += 1\n",
    "    print(f\"Converted {len(cytoscape_edges)} edges.\")\n",
    "    \n",
    "    # Combine into the final structure\n",
    "    cytoscape_graph_data = {'nodes': cytoscape_nodes, 'edges': cytoscape_edges}\n",
    "    \n",
    "    # Visualize the converted structure (first few nodes/edges)\n",
    "    print(\"\\n--- Sample Cytoscape Node Data (First 2) ---\")\n",
    "    print(json.dumps(cytoscape_graph_data['nodes'][:2], indent=2))\n",
    "    print(\"\\n--- Sample Cytoscape Edge Data (First 2) ---\")\n",
    "    print(json.dumps(cytoscape_graph_data['edges'][:2], indent=2))\n",
    "    print(\"-\" * 25)\n",
    "else:\n",
    "     print(\"Skipping data conversion as graph is not valid for visualization.\")\n",
    "     cytoscape_graph_data = {'nodes': [], 'edges': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30d90d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ipycytoscape widget...\n",
      "Widget created.\n",
      "Loading graph data into widget...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "if can_visualize:\n",
    "    print(\"Creating ipycytoscape widget...\")\n",
    "    cyto_widget = ipycytoscape.CytoscapeWidget()\n",
    "    print(\"Widget created.\")\n",
    "    \n",
    "    print(\"Loading graph data into widget...\")\n",
    "    cyto_widget.graph.add_graph_from_json(cytoscape_graph_data, directed=True)\n",
    "    print(\"Data loaded.\")\n",
    "else:\n",
    "    print(\"Skipping widget creation.\")\n",
    "    cyto_widget = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34ee9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining enhanced colorful and interactive visual style...\n",
      "Setting enhanced visual style on widget...\n",
      "Enhanced colorful and interactive style applied successfully.\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Defining enhanced colorful and interactive visual style...\")\n",
    "    # More vibrant and colorful styling with a modern color scheme\n",
    "    visual_style = [\n",
    "        {\n",
    "            'selector': 'node',\n",
    "            'style': {\n",
    "                'label': 'data(label)',\n",
    "                'width': 'data(size)',\n",
    "                'height': 'data(size)',\n",
    "                'background-color': '#3498db',  # Bright blue\n",
    "                'background-opacity': 0.9,\n",
    "                'color': '#ffffff',             # White text\n",
    "                'font-size': '12px',\n",
    "                'font-weight': 'bold',\n",
    "                'text-valign': 'center',\n",
    "                'text-halign': 'center',\n",
    "                'text-wrap': 'wrap',\n",
    "                'text-max-width': '100px',\n",
    "                'text-outline-width': 2,\n",
    "                'text-outline-color': '#2980b9',  # Matching outline\n",
    "                'text-outline-opacity': 0.7,\n",
    "                'border-width': 3,\n",
    "                'border-color': '#1abc9c',      # Turquoise border\n",
    "                'border-opacity': 0.9,\n",
    "                'shape': 'ellipse',\n",
    "                'transition-property': 'background-color, border-color, border-width, width, height',\n",
    "                'transition-duration': '0.3s',\n",
    "                'tooltip-text': 'data(tooltip_text)'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'node:selected',\n",
    "            'style': {\n",
    "                'background-color': '#e74c3c',  # Pomegranate red\n",
    "                'border-width': 4,\n",
    "                'border-color': '#c0392b',\n",
    "                'text-outline-color': '#e74c3c',\n",
    "                'width': 'data(size) * 1.2',    # Enlarge selected nodes\n",
    "                'height': 'data(size) * 1.2'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'node:hover',\n",
    "            'style': {\n",
    "                'background-color': '#9b59b6',  # Purple on hover\n",
    "                'border-width': 4,\n",
    "                'border-color': '#8e44ad',\n",
    "                'cursor': 'pointer',\n",
    "                'z-index': 999\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge',\n",
    "            'style': {\n",
    "                'label': 'data(label)',\n",
    "                'width': 2.5,\n",
    "                'curve-style': 'bezier',\n",
    "                'line-color': '#2ecc71',         # Green\n",
    "                'line-opacity': 0.8,\n",
    "                'target-arrow-color': '#27ae60',\n",
    "                'target-arrow-shape': 'triangle',\n",
    "                'arrow-scale': 1.5,\n",
    "                'font-size': '10px',\n",
    "                'font-weight': 'normal',\n",
    "                'color': '#2c3e50',\n",
    "                'text-background-opacity': 0.9,\n",
    "                'text-background-color': '#ecf0f1',\n",
    "                'text-background-shape': 'roundrectangle',\n",
    "                'text-background-padding': '3px',\n",
    "                'text-rotation': 'autorotate',\n",
    "                'edge-text-rotation': 'autorotate',\n",
    "                'transition-property': 'line-color, width, target-arrow-color',\n",
    "                'transition-duration': '0.3s',\n",
    "                'tooltip-text': 'data(tooltip_text)'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge:selected',\n",
    "            'style': {\n",
    "                'line-color': '#f39c12',         # Yellow-orange\n",
    "                'target-arrow-color': '#d35400',\n",
    "                'width': 4,\n",
    "                'text-background-color': '#f1c40f',\n",
    "                'color': '#ffffff',               # White text\n",
    "                'z-index': 998\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge:hover',\n",
    "            'style': {\n",
    "                'line-color': '#e67e22',         # Orange on hover\n",
    "                'width': 3.5,\n",
    "                'cursor': 'pointer',\n",
    "                'target-arrow-color': '#d35400',\n",
    "                'z-index': 997\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': '.center-node',\n",
    "            'style': {\n",
    "                'background-color': '#16a085',    # Teal\n",
    "                'background-opacity': 1,\n",
    "                'border-width': 4,\n",
    "                'border-color': '#1abc9c',        # Turquoise border\n",
    "                'border-opacity': 1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Setting enhanced visual style on widget...\")\n",
    "    cyto_widget.set_style(visual_style)\n",
    "    \n",
    "    # Apply a better animated layout\n",
    "    cyto_widget.set_layout(name='cose', \n",
    "                          nodeRepulsion=5000, \n",
    "                          nodeOverlap=40, \n",
    "                          idealEdgeLength=120, \n",
    "                          edgeElasticity=200, \n",
    "                          nestingFactor=6, \n",
    "                          gravity=90, \n",
    "                          numIter=2500,\n",
    "                          animate=True,\n",
    "                          animationDuration=1000,\n",
    "                          initialTemp=300,\n",
    "                          coolingFactor=0.95)\n",
    "    \n",
    "    # Add a special class to main nodes (Marie Curie)\n",
    "    if len(cyto_widget.graph.nodes) > 0:\n",
    "        main_nodes = [node.data['id'] for node in cyto_widget.graph.nodes \n",
    "                     if node.data.get('degree', 0) > 10]\n",
    "        \n",
    "        # Create gradient styles for center nodes\n",
    "        for i, node_id in enumerate(main_nodes):\n",
    "            # Use vibrant colors for center nodes\n",
    "            center_style = {\n",
    "                'selector': f'node[id = \"{node_id}\"]',\n",
    "                'style': {\n",
    "                    'background-color': '#9b59b6',   # Purple\n",
    "                    'background-opacity': 0.95,\n",
    "                    'border-width': 4,\n",
    "                    'border-color': '#8e44ad',      # Darker purple border\n",
    "                    'border-opacity': 1,\n",
    "                    'text-outline-width': 3,\n",
    "                    'text-outline-color': '#8e44ad',\n",
    "                    'font-size': '14px'\n",
    "                }\n",
    "            }\n",
    "            visual_style.append(center_style)\n",
    "        \n",
    "        # Update the style with the new additions\n",
    "        cyto_widget.set_style(visual_style)\n",
    "    \n",
    "    print(\"Enhanced colorful and interactive style applied successfully.\")\n",
    "else:\n",
    "    print(\"Skipping style definition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32a56d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting layout algorithm ('cose')...\n",
      "Layout set. The graph will arrange itself when displayed.\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Setting layout algorithm ('cose')...\")\n",
    "    # cose (Compound Spring Embedder) is often good for exploring connections\n",
    "    cyto_widget.set_layout(name='cose', \n",
    "                           animate=True, \n",
    "                           # Adjust parameters for better spacing/layout\n",
    "                           nodeRepulsion=4000, # Increase repulsion \n",
    "                           nodeOverlap=40,    # Increase overlap avoidance\n",
    "                           idealEdgeLength=120, # Slightly longer ideal edges\n",
    "                           edgeElasticity=150, \n",
    "                           nestingFactor=5, \n",
    "                           gravity=100,        # Increase gravity slightly\n",
    "                           numIter=1500,      # More iterations\n",
    "                           initialTemp=200,\n",
    "                           coolingFactor=0.95,\n",
    "                           minTemp=1.0)\n",
    "    print(\"Layout set. The graph will arrange itself when displayed.\")\n",
    "else:\n",
    "     print(\"Skipping layout setting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5409f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying interactive graph widget below...\n",
      "Interact: Zoom (scroll), Pan (drag background), Move Nodes (drag nodes), Hover for details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a78d42dc1fc43ee9fd5e8fe9f02000c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cose', 'nodeRepulsion': 4000, 'nodeOverlap': 40, 'idealEdgeLength':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "End of Visualization Step.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Displaying interactive graph widget below...\")\n",
    "    print(\"Interact: Zoom (scroll), Pan (drag background), Move Nodes (drag nodes), Hover for details.\")\n",
    "    display(cyto_widget)\n",
    "else:\n",
    "    print(\"No widget to display.\")\n",
    "\n",
    "# Add a clear separator\n",
    "print(\"\\n\" + \"-\" * 25 + \"\\nEnd of Visualization Step.\" + \"\\n\" + \"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if len(chunks) > 0:\n",
    "#     for chunk_index, chunk in enumerate(chunks):\n",
    "#         user_prompt = extraction_user_prompt_template.format(text_chunk=chunk[\"text\"])\n",
    "#         full_prompt = f\"{extraction_system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
    "\n",
    "#         # Tokenize and generate\n",
    "#         inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(device)\n",
    "\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=llm_max_tokens,\n",
    "#             do_sample=False, # Greedy decoding\n",
    "#         )\n",
    "\n",
    "#         raw = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "#         print(f\"Chunk {chunk['chunk_number']} raw response: {raw}\")\n",
    "\n",
    "#     # Try JSON parsing directly or via regex fallback\n",
    "#     try:\n",
    "#         data = json.loads(raw)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = next((v for v in data.values() if isinstance(v, list)), [])\n",
    "#     except Exception:\n",
    "#         match = re.search(r'(\\[.*\\])', raw, re.DOTALL)\n",
    "#         data = json.loads(match.group(1)) if match else []\n",
    "\n",
    "#     # Validate and store triples\n",
    "#     triples = [\n",
    "#         dict(t, chunk=chunk[\"chunk_number\"])\n",
    "#         for t in data\n",
    "#         if isinstance(t, dict) and all(k in t and isinstance(t[k], str) for k in [\"subject\", \"predicate\", \"object\"])\n",
    "#     ]\n",
    "\n",
    "#     if triples:\n",
    "#         all_extracted_triples.extend(triples)\n",
    "#     else:\n",
    "#         failed_chunks.append({'chunk_number': chunk[\"chunk_number\"], 'error': 'No valid triples', 'response': raw})\n",
    "\n",
    "# print(\"Extracted triples:\")\n",
    "# print(all_extracted_triples)\n",
    "\n",
    "# if failed_chunks:\n",
    "#     print(\"\\nFailed Chunks:\")\n",
    "#     print(failed_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
