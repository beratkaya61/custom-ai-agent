{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fe007f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai             # For LLM interaction\n",
    "import json               # For parsing LLM responses\n",
    "import networkx as nx     # For creating and managing the graph data structure\n",
    "import ipycytoscape       # For interactive in-notebook graph visualization\n",
    "import ipywidgets         # For interactive elements\n",
    "import pandas as pd       # For displaying data in tables\n",
    "import os                 # For accessing environment variables (safer for API keys)\n",
    "import math               # For basic math operations\n",
    "import re                 # For basic text cleaning (regular expressions)\n",
    "import warnings           # To suppress potential deprecation warnings\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e84bba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructured_text = \"\"\"\n",
    "Marie Curie, born Maria Sk≈Çodowska in Warsaw, Poland, was a pioneering physicist and chemist.\n",
    "She conducted groundbreaking research on radioactivity. Together with her husband, Pierre Curie,\n",
    "she discovered the elements polonium and radium. Marie Curie was the first woman to win a Nobel Prize,\n",
    "the first person and only woman to win the Nobel Prize twice, and the only person to win the Nobel Prize\n",
    "in two different scientific fields. She won the Nobel Prize in Physics in 1903 with Pierre Curie\n",
    "and Henri Becquerel. Later, she won the Nobel Prize in Chemistry in 1911 for her work on radium and\n",
    "polonium. During World War I, she developed mobile radiography units, known as 'petites Curies',\n",
    "to provide X-ray services to field hospitals. Marie Curie died in 1934 from aplastic anemia, likely\n",
    "caused by her long-term exposure to radiation.\n",
    "\n",
    "Marie was born on November 7, 1867, to a family of teachers who valued education. She received her\n",
    "early schooling in Warsaw but moved to Paris in 1891 to continue her studies at the Sorbonne, where\n",
    "she earned degrees in physics and mathematics. She met Pierre Curie, a professor of physics, in 1894, \n",
    "and they married in 1895, beginning a productive scientific partnership. Following Pierre's tragic \n",
    "death in a street accident in 1906, Marie took over his teaching position, becoming the first female \n",
    "professor at the Sorbonne.\n",
    "\n",
    "The Curies' work on radioactivity was conducted in challenging conditions, in a poorly equipped shed \n",
    "with no proper ventilation, as they processed tons of pitchblende ore to isolate radium. Marie Curie\n",
    "established the Curie Institute in Paris, which became a major center for medical research. She had\n",
    "two daughters: Ir√®ne, who later won a Nobel Prize in Chemistry with her husband, and Eve, who became\n",
    "a writer. Marie's notebooks are still radioactive today and are kept in lead-lined boxes. Her legacy\n",
    "includes not only her scientific discoveries but also her role in breaking gender barriers in academia\n",
    "and science.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "93ed5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructured_text2 = \"\"\"\n",
    "\n",
    "The Battle of √áanakkale, also known as the Gallipoli Campaign (1915), was a defining moment in World War I that took place on the Gallipoli Peninsula in Turkey. \n",
    "The Allied Powers, mainly Britain and France, attempted to force a passage through the Dardanelles Strait to capture Constantinople (Istanbul) and open a supply route to Russia. \n",
    "However, they faced fierce resistance from the Ottoman Empire, which turned the tide with strategic defense and high morale.\n",
    "\n",
    "Key figures in the battle include Mustafa Kemal Atat√ºrk, then a young Ottoman commander, who played a critical role in organizing the defense at Anafartalar and Conkbayƒ±rƒ±. His leadership and famous command, ‚ÄúI do not order you to attack; I order you to die,‚Äù became legendary and cemented his status as a national hero. On the Allied side, commanders like General Ian Hamilton struggled with underestimating the terrain and Ottoman resistance, leading to heavy casualties.\n",
    "\n",
    "The campaign ended in failure for the Allies after months of stalemate, suffering over 250,000 casualties on each side. For the Ottomans, the victory became a symbol of national pride and resistance. For Australia and New Zealand, whose ANZAC troops fought valiantly, it marked a tragic but formative experience that is commemorated annually on ANZAC Day, April 25. The battle significantly shaped Turkish national identity and contributed to the eventual founding of the Republic of Turkey under Atat√ºrk.\n",
    "\n",
    "The Gallipoli Campaign is remembered for its strategic blunders, the harsh conditions faced by soldiers, and the heroism displayed on both sides. The campaign's failure led to a reevaluation of Allied strategies in the war and had lasting implications for the Middle East and the post-war world order.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f776329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking configuration is valid.\n"
     ]
    }
   ],
   "source": [
    "# --- Chunking Configuration ---\n",
    "chunk_size = 100  # Number of words per chunk (adjust as needed)\n",
    "overlap = 20     # Number of words to overlap (must be < chunk_size)\n",
    "\n",
    "if overlap >= chunk_size and chunk_size > 0:\n",
    "    raise SystemExit(\"Chunking configuration error.\")\n",
    "else:\n",
    "    print(\"Chunking configuration is valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9305cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 275 words.\n",
      "First 20 words: ['The', 'Battle', 'of', '√áanakkale,', 'also', 'known', 'as', 'the', 'Gallipoli', 'Campaign', '(1915),', 'was', 'a', 'defining', 'moment', 'in', 'World', 'War', 'I', 'that']\n"
     ]
    }
   ],
   "source": [
    "words = unstructured_text2.split()\n",
    "total_words = len(words)\n",
    "\n",
    "print(f\"Text split into {total_words} words.\")\n",
    "# Visualize the first 20 words\n",
    "print(f\"First 20 words: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "abe9765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text successfully split into 4 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "start_index = 0\n",
    "chunk_number = 1\n",
    "\n",
    "while start_index < total_words:\n",
    "    end_index = min(start_index + chunk_size, total_words)\n",
    "    chunk_text = \" \".join(words[start_index:end_index])\n",
    "    chunks.append({\"text\": chunk_text, \"chunk_number\": chunk_number})\n",
    "\n",
    "    # Calculate the start of the next chunk\n",
    "    next_start_index = start_index + chunk_size - overlap\n",
    "\n",
    "    # Ensure progress is made\n",
    "    if next_start_index <= start_index:\n",
    "        if end_index == total_words:\n",
    "             break # Already processed the last part\n",
    "        next_start_index = start_index + 1\n",
    "\n",
    "    start_index = next_start_index\n",
    "    chunk_number += 1\n",
    "\n",
    "    # Safety break (optional)\n",
    "    if chunk_number > total_words: # Simple safety\n",
    "        print(\"Warning: Chunking loop exceeded total word count, breaking.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nText successfully split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "42faad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>word_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>The Battle of √áanakkale, also known as the Gal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>Mustafa Kemal Atat√ºrk, then a young Ottoman co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>stalemate, suffering over 250,000 casualties o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>by soldiers, and the heroism displayed on both...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_number  word_count                                               text\n",
       "0             1         100  The Battle of √áanakkale, also known as the Gal...\n",
       "1             2         100  Mustafa Kemal Atat√ºrk, then a young Ottoman co...\n",
       "2             3         100  stalemate, suffering over 250,000 casualties o...\n",
       "3             4          35  by soldiers, and the heroism displayed on both..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    # Create a DataFrame for better visualization\n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    chunks_df['word_count'] = chunks_df['text'].apply(lambda x: len(x.split()))\n",
    "    display(chunks_df[['chunk_number', 'word_count', 'text']])\n",
    "else:\n",
    "    print(\"No chunks were created (text might be shorter than chunk size).\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4885fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   - **Text Chunk:** Marie Curie discovered Radium in 1898.\n",
    "\n",
    "#   - **System Prompt:** You are an expert in information extraction.\n",
    "\n",
    "#   - **User Prompt:** Extract SPO triples. Rules:\n",
    "#   - Follow the pattern.\n",
    "#   - Text: text__chunk_placeholder.\n",
    "#   - Required JSON format: Your JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "40045eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System Prompt: Sets the context/role for the LLM ---\n",
    "extraction_system_prompt = \"\"\"\n",
    "You are an AI expert specialized in knowledge graph extraction.\n",
    "Your task is to identify and extract factual Subject-Predicate-Object (SPO) triples from the given text.\n",
    "Focus on accuracy and adhere strictly to the JSON output format requested in the user prompt.\n",
    "Extract core entities and the most direct relationship.\n",
    "\"\"\"\n",
    "\n",
    "# --- User Prompt Template: Contains specific instructions and the text ---\n",
    "extraction_user_prompt_template = \"\"\"\n",
    "Please extract Subject-Predicate-Object (S-P-O) triples from the text below.\n",
    "\n",
    "**STRICT INSTRUCTIONS ‚Äì FOLLOW EXACTLY:**\n",
    "\n",
    "1.  **Output Format:** Respond ONLY with a single, valid JSON array. Each element MUST be an object with keys \"subject\", \"predicate\", \"object\".\n",
    "2.  **JSON Only:** Do NOT include any text before or after the JSON array (e.g., no 'Here is the JSON:' or explanations). Do NOT use markdown ```json ... ``` tags.\n",
    "3. **NO NESTED OBJECTS:** If `object` is a dictionary or array, convert it into flat text. Use the most specific and informative string. For example:\n",
    "   - ‚úÖ \"object\": \"nobel prize in chemistry\" instead of ‚ùå \"object\": {{ \"type\": \"nobel prize\", \"subtype\": [\"chemistry\"] }}\n",
    "   - ‚úÖ \"object\": \"radium\" instead of ‚ùå \"object\": {{ \"type\": [\"element\", \"radium\"] }}\n",
    "4. **Only Triples:** Do NOT include additional keys like `\"subject2\"`, `\"type\"`, `\"name\"`, etc.\n",
    "4.  **Concise Predicates:** Keep the 'predicate' value concise (1-3 words, ideally 1-2). Use verbs or short verb phrases (e.g., 'discovered', 'was born in', 'won').\n",
    "5.  **Lowercase:** ALL values for 'subject', 'predicate', and 'object' MUST be lowercase.\n",
    "6.  **Pronoun Resolution:** Replace pronouns (she, he, it, her, etc.) with the specific lowercase entity name they refer to based on the text context (e.g., 'marie curie').\n",
    "7.  **Specificity:** Capture specific details (e.g., 'nobel prize in physics' instead of just 'nobel prize' if specified).\n",
    "8.  **Completeness:** Extract all distinct factual relationships mentioned and triples present in the text.\n",
    "\n",
    "**Text to Process:**\n",
    "{text_chunk}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b567c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Chunks\n",
    "#       |\n",
    "# Format Prompt\n",
    "# System + User + Chunk\n",
    "#       |\n",
    "# Send to LLM API\n",
    "#       |\n",
    "# Receive Response\n",
    "#       |\n",
    "# Parse JSON\n",
    "#    /      \\\n",
    "# Validate Triples   if failure\n",
    "#    /     \\        \n",
    "# if invalid    Handle Errors / Failures\n",
    "#    \\     /\n",
    "# Store Valid Triples\n",
    "#       |\n",
    "#     (loop back to Text Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5045388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting triple extraction from 4 chunks using model './mistral-7b-instruct-v0.2.Q4_K_M.gguf'...\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results and failures\n",
    "all_extracted_triples = []\n",
    "failed_chunks = []\n",
    "\n",
    "chunk_index = 0  # Process first chunk only\n",
    "\n",
    "llm_model_path = \"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "llm_max_tokens = 512  # Allowing 1536 tokens for prompt, 512 for generation\n",
    "\n",
    "print(\n",
    "    f\"Starting triple extraction from {len(chunks)} chunks using model '{llm_model_path}'...\"\n",
    ")\n",
    "# We will process chunks one by one in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "323b023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 32\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 32, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 32, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 32, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    10.50 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM model\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=llm_model_path,\n",
    "    n_ctx=2048,  #  if prompt + response exceeds 1024 tokens, the model truncates the output mid-JSON.\n",
    "    n_threads=16,  # Use 16 of your 20 logical threads for good performance\n",
    "    verbose=True,  # Optional: Show debug info\n",
    "    n_batch=32,  # Batch size for processing, Keep moderate; increase only if RAM allows (64)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408db89a",
   "metadata": {},
   "source": [
    "üí° If You Still Get Incomplete Output\n",
    "Reduce chunk_size in text splitter (e.g., from 150 to 100 words).\n",
    "\n",
    "Shorten your prompt wording to conserve tokens.\n",
    "\n",
    "Stream and truncate at the first ] as discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a7190633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_parse_llm_output(llm_output_raw: str, chunk_num: int):\n",
    "    cleaned = re.sub(r\"(?i)^```json|```\", \"\", llm_output_raw.strip(), flags=re.IGNORECASE).strip()\n",
    "    if cleaned.count(\"[\") > cleaned.count(\"]\"):\n",
    "        cleaned += \"]\"\n",
    "\n",
    "    # print(f\"[Chunk {chunk_num}] Cleaned LLM output: {cleaned}...\") \n",
    "\n",
    "    # Step 2: Extract only the first JSON array using regex\n",
    "    match = re.search(r\"\\[\\s*{.*?}\\s*\\]\", cleaned, re.DOTALL)\n",
    "    if match:\n",
    "        json_part = match.group(0)\n",
    "    else:\n",
    "        print(f\"[Chunk {chunk_num}] ‚ùå ERROR: No JSON array detected.\")\n",
    "        return []\n",
    "\n",
    "    # Save Cleaned LLM output to a txt file (append mode)\n",
    "    with open(\"debug_llm_output.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n--- Chunk {chunk_num} ---\\n\")\n",
    "        f.write(json_part + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(json_part)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[Chunk {chunk_num}] JSONDecodeError: {e}\")\n",
    "        return []\n",
    "\n",
    "    triples = []\n",
    "    def extract(item, parent_subject=None):\n",
    "        if not isinstance(item, dict): return\n",
    "        subj = item.get(\"subject\") or parent_subject\n",
    "        pred = item.get(\"predicate\")\n",
    "        obj = item.get(\"object\")\n",
    "\n",
    "        if isinstance(obj, str):\n",
    "            triples.append({\"subject\": subj, \"predicate\": pred, \"object\": obj})\n",
    "        elif isinstance(obj, dict):\n",
    "            for key, val in obj.items():\n",
    "                if isinstance(val, list):\n",
    "                    for v in val:\n",
    "                        extract(v, parent_subject=v.get(\"name\", subj))\n",
    "        elif isinstance(obj, list):\n",
    "            for v in obj:\n",
    "                extract(v, parent_subject=subj)\n",
    "\n",
    "    if isinstance(parsed, list):\n",
    "        for item in parsed:\n",
    "            extract(item)\n",
    "    elif isinstance(parsed, dict):\n",
    "        extract(parsed)\n",
    "\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1db3a0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 1/4 --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   26482.90 ms\n",
      "llama_perf_context_print: prompt eval time =   26479.68 ms /   699 tokens (   37.88 ms per token,    26.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40613.11 ms /   367 runs   (  110.66 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   67327.31 ms /  1066 tokens\n",
      "Llama.generate: 547 prefix-match hit, remaining 154 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Processing Chunk 2/4 --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   26482.90 ms\n",
      "llama_perf_context_print: prompt eval time =    4056.40 ms /   154 tokens (   26.34 ms per token,    37.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46597.10 ms /   416 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   50895.19 ms /   570 tokens\n",
      "Llama.generate: 547 prefix-match hit, remaining 149 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Processing Chunk 3/4 --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   26482.90 ms\n",
      "llama_perf_context_print: prompt eval time =    4009.49 ms /   149 tokens (   26.91 ms per token,    37.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50741.26 ms /   459 runs   (  110.55 ms per token,     9.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   55063.44 ms /   608 tokens\n",
      "Llama.generate: 547 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Processing Chunk 4/4 --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   26482.90 ms\n",
      "llama_perf_context_print: prompt eval time =    1419.31 ms /    46 tokens (   30.85 ms per token,    32.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45756.67 ms /   415 runs   (  110.26 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   47430.87 ms /   461 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 4] JSONDecodeError: Expecting property name enclosed in double quotes: line 35 column 3 (char 674)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " all_extracted_triples: [{'subject': 'the battle of √ßanakkale', 'predicate': 'took place', 'object': 'from february 19, 1915, to january 9, 1916'}, {'subject': 'the allied powers', 'predicate': 'attempted to force a passage through', 'object': 'the dardanelles strait'}, {'subject': 'the ottoman empire', 'predicate': 'faced fierce resistance from', 'object': 'the allied powers'}, {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'played a critical role in organizing the defense at', 'object': 'anafartalar'}, {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'later became the founder of', 'object': 'modern turkey'}, {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'played_a_critical_role_in', 'object': 'organizing_the_defense_at_anafartalar_and_conkbayƒ±rƒ±'}, {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'became_legendary_for', 'object': 'his_command_i_do_not_order_you_to_attack_i_order_you_to_die'}, {'subject': 'general_ian_hamilton', 'predicate': 'struggled_with', 'object': 'underestimating_the_terrain_and_ottoman_resistance'}, {'subject': 'allied_side', 'predicate': 'suffered_over', 'object': '250000_casualties'}, {'subject': 'ottomans', 'predicate': 'suffered_over', 'object': '250000_casualties'}, {'subject': 'allied_side', 'predicate': 'ended_in_failure_after', 'object': 'months_of_stalemate'}, {'subject': 'gallipoli campaign', 'predicate': 'marked', 'object': 'a tragic but formative experience for australia and new zealand'}, {'subject': 'gallipoli campaign', 'predicate': 'significantly shaped', 'object': 'turkish national identity'}, {'subject': 'gallipoli campaign', 'predicate': 'contributed to', 'object': 'the eventual founding of the republic of turkey under atat√ºrk'}, {'subject': 'gallipoli campaign', 'predicate': 'is remembered for', 'object': 'its strategic blunders, harsh conditions, and heroism'}, {'subject': 'gallipoli campaign', 'predicate': 'led to', 'object': 'a reevaluation of allied strategies in the first world war'}, {'subject': 'ottoman empire', 'predicate': 'suffered', 'object': 'over 250,000 casualties'}, {'subject': 'ottoman empire', 'predicate': 'viewed', 'object': 'the victory as a symbol of national pride and resistance'}, {'subject': 'anzac troops', 'predicate': 'fought', 'object': 'valiantly'}, {'subject': 'anzac day', 'predicate': 'commemorated annually', 'object': 'april 25'}, {'subject': 'atat√ºrk', 'predicate': 'founded', 'object': 'the republic of turkey'}] triples extracted.\n"
     ]
    }
   ],
   "source": [
    "for chunk_index, chunk_info in enumerate(chunks):\n",
    "\n",
    "    chunk_text = chunk_info[\"text\"]\n",
    "    chunk_num = chunk_info[\"chunk_number\"]\n",
    "\n",
    "    print(f\"\\n--- Processing Chunk {chunk_num}/{len(chunks)} --- \")\n",
    "\n",
    "    # 1. Format the User Prompt\n",
    "    user_prompt = extraction_user_prompt_template.format(text_chunk=chunk_text)\n",
    "    full_prompt = f\"{extraction_system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
    "\n",
    "    llm_output = None\n",
    "    error_message = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        # 2. Send the formatted prompt to the LLM API\n",
    "        response = llm.create_completion(\n",
    "            prompt=full_prompt,\n",
    "            max_tokens=llm_max_tokens,  # allow the model to generate enough tokens for the response\n",
    "            temperature=0.0,\n",
    "            stop=[\"</s>\"],\n",
    "        )\n",
    "\n",
    "        # 3. Extract Raw Response Content\n",
    "        raw_chunks = []\n",
    "\n",
    "        if response and \"choices\" in response:\n",
    "            for choice in response[\"choices\"]:\n",
    "                if choice and \"text\" in choice:\n",
    "                    raw_chunks.append(choice[\"text\"])\n",
    "\n",
    "        # Combine chunks into a single string\n",
    "        llm_output = \"\".join(raw_chunks).strip()\n",
    "\n",
    "        # Clean and parse the JSON-like output\n",
    "        triples = clean_and_parse_llm_output(llm_output, chunk_num)\n",
    "        \n",
    "        all_extracted_triples.extend(triples)\n",
    "\n",
    "        print(\"-\" * 200)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"[Chunk {chunk_num}] Error: {error_message}\")\n",
    "\n",
    "\n",
    "print(f\"\\n all_extracted_triples: {all_extracted_triples} triples extracted.\")\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fe76a4f7-c173-452a-a93e-d774e7bf2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Extraction Summary ---\n",
      "Total chunks defined: 4\n",
      "Chunks processed (attempted): 4\n",
      "Total valid triples extracted across all processed chunks: 21\n",
      "Number of chunks that failed API call or parsing: 0\n",
      "-------------------------\n",
      "\n",
      "--- All Extracted Triples (Before Normalization) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the battle of √ßanakkale</td>\n",
       "      <td>took place</td>\n",
       "      <td>from february 19, 1915, to january 9, 1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the allied powers</td>\n",
       "      <td>attempted to force a passage through</td>\n",
       "      <td>the dardanelles strait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ottoman empire</td>\n",
       "      <td>faced fierce resistance from</td>\n",
       "      <td>the allied powers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>played a critical role in organizing the defen...</td>\n",
       "      <td>anafartalar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>later became the founder of</td>\n",
       "      <td>modern turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>played_a_critical_role_in</td>\n",
       "      <td>organizing_the_defense_at_anafartalar_and_conk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>became_legendary_for</td>\n",
       "      <td>his_command_i_do_not_order_you_to_attack_i_ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>general_ian_hamilton</td>\n",
       "      <td>struggled_with</td>\n",
       "      <td>underestimating_the_terrain_and_ottoman_resist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>suffered_over</td>\n",
       "      <td>250000_casualties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ottomans</td>\n",
       "      <td>suffered_over</td>\n",
       "      <td>250000_casualties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>ended_in_failure_after</td>\n",
       "      <td>months_of_stalemate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>marked</td>\n",
       "      <td>a tragic but formative experience for australi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>significantly shaped</td>\n",
       "      <td>turkish national identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>contributed to</td>\n",
       "      <td>the eventual founding of the republic of turke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>is remembered for</td>\n",
       "      <td>its strategic blunders, harsh conditions, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>led to</td>\n",
       "      <td>a reevaluation of allied strategies in the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ottoman empire</td>\n",
       "      <td>suffered</td>\n",
       "      <td>over 250,000 casualties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ottoman empire</td>\n",
       "      <td>viewed</td>\n",
       "      <td>the victory as a symbol of national pride and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>anzac troops</td>\n",
       "      <td>fought</td>\n",
       "      <td>valiantly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>anzac day</td>\n",
       "      <td>commemorated annually</td>\n",
       "      <td>april 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>atat√ºrk</td>\n",
       "      <td>founded</td>\n",
       "      <td>the republic of turkey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject  \\\n",
       "0   the battle of √ßanakkale   \n",
       "1         the allied powers   \n",
       "2        the ottoman empire   \n",
       "3     mustafa kemal atat√ºrk   \n",
       "4     mustafa kemal atat√ºrk   \n",
       "5     mustafa kemal atat√ºrk   \n",
       "6     mustafa kemal atat√ºrk   \n",
       "7      general_ian_hamilton   \n",
       "8               allied_side   \n",
       "9                  ottomans   \n",
       "10              allied_side   \n",
       "11       gallipoli campaign   \n",
       "12       gallipoli campaign   \n",
       "13       gallipoli campaign   \n",
       "14       gallipoli campaign   \n",
       "15       gallipoli campaign   \n",
       "16           ottoman empire   \n",
       "17           ottoman empire   \n",
       "18             anzac troops   \n",
       "19                anzac day   \n",
       "20                  atat√ºrk   \n",
       "\n",
       "                                            predicate  \\\n",
       "0                                          took place   \n",
       "1                attempted to force a passage through   \n",
       "2                        faced fierce resistance from   \n",
       "3   played a critical role in organizing the defen...   \n",
       "4                         later became the founder of   \n",
       "5                           played_a_critical_role_in   \n",
       "6                                became_legendary_for   \n",
       "7                                      struggled_with   \n",
       "8                                       suffered_over   \n",
       "9                                       suffered_over   \n",
       "10                             ended_in_failure_after   \n",
       "11                                             marked   \n",
       "12                               significantly shaped   \n",
       "13                                     contributed to   \n",
       "14                                  is remembered for   \n",
       "15                                             led to   \n",
       "16                                           suffered   \n",
       "17                                             viewed   \n",
       "18                                             fought   \n",
       "19                              commemorated annually   \n",
       "20                                            founded   \n",
       "\n",
       "                                               object  \n",
       "0          from february 19, 1915, to january 9, 1916  \n",
       "1                              the dardanelles strait  \n",
       "2                                   the allied powers  \n",
       "3                                         anafartalar  \n",
       "4                                       modern turkey  \n",
       "5   organizing_the_defense_at_anafartalar_and_conk...  \n",
       "6   his_command_i_do_not_order_you_to_attack_i_ord...  \n",
       "7   underestimating_the_terrain_and_ottoman_resist...  \n",
       "8                                   250000_casualties  \n",
       "9                                   250000_casualties  \n",
       "10                                months_of_stalemate  \n",
       "11  a tragic but formative experience for australi...  \n",
       "12                          turkish national identity  \n",
       "13  the eventual founding of the republic of turke...  \n",
       "14  its strategic blunders, harsh conditions, and ...  \n",
       "15  a reevaluation of allied strategies in the fir...  \n",
       "16                            over 250,000 casualties  \n",
       "17  the victory as a symbol of national pride and ...  \n",
       "18                                          valiantly  \n",
       "19                                           april 25  \n",
       "20                             the republic of turkey  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Summary of Extraction (Reflecting state after the single chunk demo) ---\n",
    "print(f\"\\n--- Overall Extraction Summary ---\")\n",
    "print(f\"Total chunks defined: {len(chunks)}\")\n",
    "processed_chunks = len(chunks) - len(failed_chunks) # Approximation if loop isn't run fully\n",
    "print(f\"Chunks processed (attempted): {processed_chunks + len(failed_chunks)}\") # Chunks we looped through\n",
    "print(f\"Total valid triples extracted across all processed chunks: {len(all_extracted_triples)}\")\n",
    "print(f\"Number of chunks that failed API call or parsing: {len(failed_chunks)}\")\n",
    "\n",
    "if failed_chunks:\n",
    "    print(\"\\nDetails of Failed Chunks:\")\n",
    "    for failure in failed_chunks:\n",
    "        print(f\"  Chunk {failure['chunk_number']}: Error: {failure['error']}\")\n",
    "        # print(f\"    Response (start): {failure.get('response', '')[:100]}...\") # Uncomment for more detail\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Display all extracted triples using Pandas\n",
    "print(\"\\n--- All Extracted Triples (Before Normalization) ---\")\n",
    "if all_extracted_triples:\n",
    "    all_triples_df = pd.DataFrame(all_extracted_triples)\n",
    "    display(all_triples_df)\n",
    "else:\n",
    "    print(\"No triples were successfully extracted.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86906aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting normalization and de-duplication of 21 triples...\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists and tracking variables\n",
    "\n",
    "# Normalize: Trim whitespace, convert to lowercase.\n",
    "# Filter: Remove triples with empty parts after normalization.\n",
    "# De-duplicate: Remove exact duplicate (subject, predicate, object) combinations.\n",
    "\n",
    "normalized_triples = []\n",
    "seen_triples = set() # Tracks (subject, predicate, object) tuples\n",
    "original_count = len(all_extracted_triples)\n",
    "empty_removed_count = 0\n",
    "duplicates_removed_count = 0\n",
    "\n",
    "print(f\"Starting normalization and de-duplication of {original_count} triples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b0bb1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing triples for normalization (showing first 5 examples):\n",
      "\n",
      "--- Example 1 ---\n",
      "Original Triple (Chunk ?): {'subject': 'the battle of √ßanakkale', 'predicate': 'took place', 'object': 'from february 19, 1915, to january 9, 1916'}\n",
      "Normalized: SUB='the battle of √ßanakkale', PRED='took place', OBJ='from february 19, 1915, to january 9, 1916'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 2 ---\n",
      "Original Triple (Chunk ?): {'subject': 'the allied powers', 'predicate': 'attempted to force a passage through', 'object': 'the dardanelles strait'}\n",
      "Normalized: SUB='the allied powers', PRED='attempted to force a passage through', OBJ='the dardanelles strait'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 3 ---\n",
      "Original Triple (Chunk ?): {'subject': 'the ottoman empire', 'predicate': 'faced fierce resistance from', 'object': 'the allied powers'}\n",
      "Normalized: SUB='the ottoman empire', PRED='faced fierce resistance from', OBJ='the allied powers'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 4 ---\n",
      "Original Triple (Chunk ?): {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'played a critical role in organizing the defense at', 'object': 'anafartalar'}\n",
      "Normalized: SUB='mustafa kemal atat√ºrk', PRED='played a critical role in organizing the defense at', OBJ='anafartalar'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "--- Example 5 ---\n",
      "Original Triple (Chunk ?): {'subject': 'mustafa kemal atat√ºrk', 'predicate': 'later became the founder of', 'object': 'modern turkey'}\n",
      "Normalized: SUB='mustafa kemal atat√ºrk', PRED='later became the founder of', OBJ='modern turkey'\n",
      "Status: Kept (New Unique Triple)\n",
      "\n",
      "... Finished processing 21 triples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing triples for normalization (showing first 5 examples):\")\n",
    "example_limit = 5\n",
    "processed_count = 0\n",
    "\n",
    "for i, triple in enumerate(all_extracted_triples):\n",
    "    show_example = (i < example_limit)\n",
    "    if show_example:\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Original Triple (Chunk {triple.get('chunk', '?')}): {triple}\")\n",
    "        \n",
    "    subject_raw = triple.get('subject')\n",
    "    predicate_raw = triple.get('predicate')\n",
    "    object_raw = triple.get('object')\n",
    "    chunk_num = triple.get('chunk', 'unknown')\n",
    "    \n",
    "    triple_valid = False\n",
    "    normalized_sub, normalized_pred, normalized_obj = None, None, None\n",
    "\n",
    "    if isinstance(subject_raw, str) and isinstance(predicate_raw, str) and isinstance(object_raw, str):\n",
    "        # 1. Normalize\n",
    "        normalized_sub = subject_raw.strip().lower()\n",
    "        normalized_pred = re.sub(r'\\s+', ' ', predicate_raw.strip().lower()).strip()\n",
    "        normalized_obj = object_raw.strip().lower()\n",
    "        if show_example:\n",
    "            print(f\"Normalized: SUB='{normalized_sub}', PRED='{normalized_pred}', OBJ='{normalized_obj}'\")\n",
    "\n",
    "        # 2. Filter Empty\n",
    "        if normalized_sub and normalized_pred and normalized_obj:\n",
    "            triple_identifier = (normalized_sub, normalized_pred, normalized_obj)\n",
    "            \n",
    "            # 3. De-duplicate\n",
    "            if triple_identifier not in seen_triples:\n",
    "                normalized_triples.append({\n",
    "                    'subject': normalized_sub,\n",
    "                    'predicate': normalized_pred,\n",
    "                    'object': normalized_obj,\n",
    "                    'source_chunk': chunk_num\n",
    "                })\n",
    "                seen_triples.add(triple_identifier)\n",
    "                triple_valid = True\n",
    "                if show_example:\n",
    "                    print(\"Status: Kept (New Unique Triple)\")\n",
    "            else:\n",
    "                duplicates_removed_count += 1\n",
    "                if show_example:\n",
    "                    print(\"Status: Discarded (Duplicate)\")\n",
    "        else:\n",
    "            empty_removed_count += 1\n",
    "            if show_example:\n",
    "                print(\"Status: Discarded (Empty component after normalization)\")\n",
    "    else:\n",
    "        empty_removed_count += 1 # Count non-string/missing as needing removal\n",
    "        if show_example:\n",
    "             print(\"Status: Discarded (Non-string or missing component)\")\n",
    "    processed_count += 1\n",
    "\n",
    "print(f\"\\n... Finished processing {processed_count} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "16ee9773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalization & De-duplication Summary ---\n",
      "Original extracted triple count: 21\n",
      "Triples removed (empty/invalid components): 0\n",
      "Duplicate triples removed: 0\n",
      "Final unique, normalized triple count: 21\n",
      "-------------------------\n",
      "\n",
      "--- Final Normalized Triples ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>source_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the battle of √ßanakkale</td>\n",
       "      <td>took place</td>\n",
       "      <td>from february 19, 1915, to january 9, 1916</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the allied powers</td>\n",
       "      <td>attempted to force a passage through</td>\n",
       "      <td>the dardanelles strait</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ottoman empire</td>\n",
       "      <td>faced fierce resistance from</td>\n",
       "      <td>the allied powers</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>played a critical role in organizing the defen...</td>\n",
       "      <td>anafartalar</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>later became the founder of</td>\n",
       "      <td>modern turkey</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>played_a_critical_role_in</td>\n",
       "      <td>organizing_the_defense_at_anafartalar_and_conk...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>became_legendary_for</td>\n",
       "      <td>his_command_i_do_not_order_you_to_attack_i_ord...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>general_ian_hamilton</td>\n",
       "      <td>struggled_with</td>\n",
       "      <td>underestimating_the_terrain_and_ottoman_resist...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>suffered_over</td>\n",
       "      <td>250000_casualties</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ottomans</td>\n",
       "      <td>suffered_over</td>\n",
       "      <td>250000_casualties</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>ended_in_failure_after</td>\n",
       "      <td>months_of_stalemate</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>marked</td>\n",
       "      <td>a tragic but formative experience for australi...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>significantly shaped</td>\n",
       "      <td>turkish national identity</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>contributed to</td>\n",
       "      <td>the eventual founding of the republic of turke...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>is remembered for</td>\n",
       "      <td>its strategic blunders, harsh conditions, and ...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gallipoli campaign</td>\n",
       "      <td>led to</td>\n",
       "      <td>a reevaluation of allied strategies in the fir...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ottoman empire</td>\n",
       "      <td>suffered</td>\n",
       "      <td>over 250,000 casualties</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ottoman empire</td>\n",
       "      <td>viewed</td>\n",
       "      <td>the victory as a symbol of national pride and ...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>anzac troops</td>\n",
       "      <td>fought</td>\n",
       "      <td>valiantly</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>anzac day</td>\n",
       "      <td>commemorated annually</td>\n",
       "      <td>april 25</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>atat√ºrk</td>\n",
       "      <td>founded</td>\n",
       "      <td>the republic of turkey</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject  \\\n",
       "0   the battle of √ßanakkale   \n",
       "1         the allied powers   \n",
       "2        the ottoman empire   \n",
       "3     mustafa kemal atat√ºrk   \n",
       "4     mustafa kemal atat√ºrk   \n",
       "5     mustafa kemal atat√ºrk   \n",
       "6     mustafa kemal atat√ºrk   \n",
       "7      general_ian_hamilton   \n",
       "8               allied_side   \n",
       "9                  ottomans   \n",
       "10              allied_side   \n",
       "11       gallipoli campaign   \n",
       "12       gallipoli campaign   \n",
       "13       gallipoli campaign   \n",
       "14       gallipoli campaign   \n",
       "15       gallipoli campaign   \n",
       "16           ottoman empire   \n",
       "17           ottoman empire   \n",
       "18             anzac troops   \n",
       "19                anzac day   \n",
       "20                  atat√ºrk   \n",
       "\n",
       "                                            predicate  \\\n",
       "0                                          took place   \n",
       "1                attempted to force a passage through   \n",
       "2                        faced fierce resistance from   \n",
       "3   played a critical role in organizing the defen...   \n",
       "4                         later became the founder of   \n",
       "5                           played_a_critical_role_in   \n",
       "6                                became_legendary_for   \n",
       "7                                      struggled_with   \n",
       "8                                       suffered_over   \n",
       "9                                       suffered_over   \n",
       "10                             ended_in_failure_after   \n",
       "11                                             marked   \n",
       "12                               significantly shaped   \n",
       "13                                     contributed to   \n",
       "14                                  is remembered for   \n",
       "15                                             led to   \n",
       "16                                           suffered   \n",
       "17                                             viewed   \n",
       "18                                             fought   \n",
       "19                              commemorated annually   \n",
       "20                                            founded   \n",
       "\n",
       "                                               object source_chunk  \n",
       "0          from february 19, 1915, to january 9, 1916      unknown  \n",
       "1                              the dardanelles strait      unknown  \n",
       "2                                   the allied powers      unknown  \n",
       "3                                         anafartalar      unknown  \n",
       "4                                       modern turkey      unknown  \n",
       "5   organizing_the_defense_at_anafartalar_and_conk...      unknown  \n",
       "6   his_command_i_do_not_order_you_to_attack_i_ord...      unknown  \n",
       "7   underestimating_the_terrain_and_ottoman_resist...      unknown  \n",
       "8                                   250000_casualties      unknown  \n",
       "9                                   250000_casualties      unknown  \n",
       "10                                months_of_stalemate      unknown  \n",
       "11  a tragic but formative experience for australi...      unknown  \n",
       "12                          turkish national identity      unknown  \n",
       "13  the eventual founding of the republic of turke...      unknown  \n",
       "14  its strategic blunders, harsh conditions, and ...      unknown  \n",
       "15  a reevaluation of allied strategies in the fir...      unknown  \n",
       "16                            over 250,000 casualties      unknown  \n",
       "17  the victory as a symbol of national pride and ...      unknown  \n",
       "18                                          valiantly      unknown  \n",
       "19                                           april 25      unknown  \n",
       "20                             the republic of turkey      unknown  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Summary of Normalization --- \n",
    "print(f\"\\n--- Normalization & De-duplication Summary ---\")\n",
    "print(f\"Original extracted triple count: {original_count}\")\n",
    "print(f\"Triples removed (empty/invalid components): {empty_removed_count}\")\n",
    "print(f\"Duplicate triples removed: {duplicates_removed_count}\")\n",
    "final_count = len(normalized_triples)\n",
    "print(f\"Final unique, normalized triple count: {final_count}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Display a sample of normalized triples using Pandas\n",
    "print(\"\\n--- Final Normalized Triples ---\")\n",
    "if normalized_triples:\n",
    "    normalized_df = pd.DataFrame(normalized_triples)\n",
    "    display(normalized_df)\n",
    "else:\n",
    "    print(\"No valid triples remain after normalization.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d3f816df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty NetworkX DiGraph.\n",
      "--- Initial Graph Info ---\n",
      "Type: DiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create an empty directed graph\n",
    "knowledge_graph = nx.DiGraph()\n",
    "\n",
    "print(\"Initialized an empty NetworkX DiGraph.\")\n",
    "# Visualize the initial empty graph state\n",
    "print(\"--- Initial Graph Info ---\")\n",
    "try:\n",
    "    # Try the newer method first\n",
    "    print(nx.info(knowledge_graph))\n",
    "except AttributeError:\n",
    "    # Fallback for different NetworkX versions\n",
    "    print(f\"Type: {type(knowledge_graph).__name__}\")\n",
    "    print(f\"Number of nodes: {knowledge_graph.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {knowledge_graph.number_of_edges()}\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b97e17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding triples to the NetworkX graph...\n",
      "\n",
      "--- Graph Info after adding Triple #5 --- (mustafa kemal atat√ºrk -> modern turkey)\n",
      "Type: DiGraph\n",
      "Number of nodes: 8\n",
      "Number of edges: 5\n",
      "\n",
      "--- Graph Info after adding Triple #10 --- (ottomans -> 250000_casualties)\n",
      "Type: DiGraph\n",
      "Number of nodes: 15\n",
      "Number of edges: 10\n",
      "\n",
      "--- Graph Info after adding Triple #15 --- (gallipoli campaign -> its strategic blunders, harsh conditions, and heroism)\n",
      "Type: DiGraph\n",
      "Number of nodes: 21\n",
      "Number of edges: 15\n",
      "\n",
      "--- Graph Info after adding Triple #20 --- (anzac day -> april 25)\n",
      "Type: DiGraph\n",
      "Number of nodes: 29\n",
      "Number of edges: 20\n",
      "\n",
      "--- Graph Info after adding Triple #21 --- (atat√ºrk -> the republic of turkey)\n",
      "Type: DiGraph\n",
      "Number of nodes: 31\n",
      "Number of edges: 21\n",
      "\n",
      "Finished adding triples. Processed 21 edges.\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding triples to the NetworkX graph...\")\n",
    "\n",
    "added_edges_count = 0\n",
    "update_interval = 5 # How often to print graph info update\n",
    "\n",
    "if not normalized_triples:\n",
    "    print(\"Warning: No normalized triples to add to the graph.\")\n",
    "else:\n",
    "    for i, triple in enumerate(normalized_triples):\n",
    "        subject_node = triple['subject']\n",
    "        object_node = triple['object']\n",
    "        predicate_label = triple['predicate']\n",
    "        \n",
    "        # Nodes are added automatically when adding edges, but explicit calls are fine too\n",
    "        knowledge_graph.add_node(subject_node) \n",
    "        knowledge_graph.add_node(object_node)\n",
    "        \n",
    "        # Add the directed edge with the predicate as a 'label' attribute\n",
    "        knowledge_graph.add_edge(subject_node, object_node, label=predicate_label)\n",
    "        added_edges_count += 1\n",
    "        \n",
    "        # --- Visualize Graph Growth --- \n",
    "        if (i + 1) % update_interval == 0 or (i + 1) == len(normalized_triples):\n",
    "            print(f\"\\n--- Graph Info after adding Triple #{i+1} --- ({subject_node} -> {object_node})\")\n",
    "            try:\n",
    "                # Try the newer method first\n",
    "                print(nx.info(knowledge_graph))\n",
    "            except AttributeError:\n",
    "                # Fallback for different NetworkX versions\n",
    "                print(f\"Type: {type(knowledge_graph).__name__}\")\n",
    "                print(f\"Number of nodes: {knowledge_graph.number_of_nodes()}\")\n",
    "                print(f\"Number of edges: {knowledge_graph.number_of_edges()}\")\n",
    "            # For very large graphs, printing info too often can be slow. Adjust interval.\n",
    "\n",
    "print(f\"\\nFinished adding triples. Processed {added_edges_count} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "37688888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final NetworkX Graph Summary ---\n",
      "Total unique nodes (entities): 31\n",
      "Total unique edges (relationships): 21\n",
      "Graph density: 0.0226\n",
      "The graph has 10 weakly connected components.\n",
      "-------------------------\n",
      "\n",
      "--- Sample Nodes (First 10) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the battle of √ßanakkale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from february 19, 1915, to january 9, 1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the allied powers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the dardanelles strait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ottoman empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>anafartalar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>modern turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>organizing_the_defense_at_anafartalar_and_conk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his_command_i_do_not_order_you_to_attack_i_ord...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Node Sample\n",
       "0                            the battle of √ßanakkale\n",
       "1         from february 19, 1915, to january 9, 1916\n",
       "2                                  the allied powers\n",
       "3                             the dardanelles strait\n",
       "4                                 the ottoman empire\n",
       "5                              mustafa kemal atat√ºrk\n",
       "6                                        anafartalar\n",
       "7                                      modern turkey\n",
       "8  organizing_the_defense_at_anafartalar_and_conk...\n",
       "9  his_command_i_do_not_order_you_to_attack_i_ord..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Edges (First 10 with Labels) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the battle of √ßanakkale</td>\n",
       "      <td>from february 19, 1915, to january 9, 1916</td>\n",
       "      <td>took place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the allied powers</td>\n",
       "      <td>the dardanelles strait</td>\n",
       "      <td>attempted to force a passage through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the ottoman empire</td>\n",
       "      <td>the allied powers</td>\n",
       "      <td>faced fierce resistance from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>anafartalar</td>\n",
       "      <td>played a critical role in organizing the defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>modern turkey</td>\n",
       "      <td>later became the founder of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>organizing_the_defense_at_anafartalar_and_conk...</td>\n",
       "      <td>played_a_critical_role_in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mustafa kemal atat√ºrk</td>\n",
       "      <td>his_command_i_do_not_order_you_to_attack_i_ord...</td>\n",
       "      <td>became_legendary_for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>general_ian_hamilton</td>\n",
       "      <td>underestimating_the_terrain_and_ottoman_resist...</td>\n",
       "      <td>struggled_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>250000_casualties</td>\n",
       "      <td>suffered_over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>allied_side</td>\n",
       "      <td>months_of_stalemate</td>\n",
       "      <td>ended_in_failure_after</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Source                                             Target  \\\n",
       "0  the battle of √ßanakkale         from february 19, 1915, to january 9, 1916   \n",
       "1        the allied powers                             the dardanelles strait   \n",
       "2       the ottoman empire                                  the allied powers   \n",
       "3    mustafa kemal atat√ºrk                                        anafartalar   \n",
       "4    mustafa kemal atat√ºrk                                      modern turkey   \n",
       "5    mustafa kemal atat√ºrk  organizing_the_defense_at_anafartalar_and_conk...   \n",
       "6    mustafa kemal atat√ºrk  his_command_i_do_not_order_you_to_attack_i_ord...   \n",
       "7     general_ian_hamilton  underestimating_the_terrain_and_ottoman_resist...   \n",
       "8              allied_side                                  250000_casualties   \n",
       "9              allied_side                                months_of_stalemate   \n",
       "\n",
       "                                               Label  \n",
       "0                                         took place  \n",
       "1               attempted to force a passage through  \n",
       "2                       faced fierce resistance from  \n",
       "3  played a critical role in organizing the defen...  \n",
       "4                        later became the founder of  \n",
       "5                          played_a_critical_role_in  \n",
       "6                               became_legendary_for  \n",
       "7                                     struggled_with  \n",
       "8                                      suffered_over  \n",
       "9                             ended_in_failure_after  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Final Graph Statistics --- \n",
    "num_nodes = knowledge_graph.number_of_nodes()\n",
    "num_edges = knowledge_graph.number_of_edges()\n",
    "\n",
    "print(f\"\\n--- Final NetworkX Graph Summary ---\")\n",
    "print(f\"Total unique nodes (entities): {num_nodes}\")\n",
    "print(f\"Total unique edges (relationships): {num_edges}\")\n",
    "\n",
    "if num_edges != added_edges_count and isinstance(knowledge_graph, nx.DiGraph):\n",
    "     print(f\"Note: Added {added_edges_count} edges, but graph has {num_edges}. DiGraph overwrites edges with same source/target. Use MultiDiGraph if multiple edges needed.\")\n",
    "\n",
    "if num_nodes > 0:\n",
    "    try:\n",
    "       density = nx.density(knowledge_graph)\n",
    "       print(f\"Graph density: {density:.4f}\")\n",
    "       if nx.is_weakly_connected(knowledge_graph):\n",
    "           print(\"The graph is weakly connected (all nodes reachable ignoring direction).\")\n",
    "       else:\n",
    "           num_components = nx.number_weakly_connected_components(knowledge_graph)\n",
    "           print(f\"The graph has {num_components} weakly connected components.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate some graph metrics: {e}\") # Handle potential errors on empty/small graphs\n",
    "else:\n",
    "    print(\"Graph is empty, cannot calculate metrics.\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- Sample Nodes --- \n",
    "print(\"\\n--- Sample Nodes (First 10) ---\")\n",
    "if num_nodes > 0:\n",
    "    nodes_sample = list(knowledge_graph.nodes())[:10]\n",
    "    display(pd.DataFrame(nodes_sample, columns=['Node Sample']))\n",
    "else:\n",
    "    print(\"Graph has no nodes.\")\n",
    "\n",
    "# --- Sample Edges --- \n",
    "print(\"\\n--- Sample Edges (First 10 with Labels) ---\")\n",
    "if num_edges > 0:\n",
    "    edges_sample = []\n",
    "    for u, v, data in list(knowledge_graph.edges(data=True))[:10]:\n",
    "        edges_sample.append({'Source': u, 'Target': v, 'Label': data.get('label', 'N/A')})\n",
    "    display(pd.DataFrame(edges_sample))\n",
    "else:\n",
    "    print(\"Graph has no edges.\")\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "67f20dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing interactive visualization...\n",
      "Graph seems valid for visualization (31 nodes, 21 edges).\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing interactive visualization...\")\n",
    "\n",
    "# --- Check Graph Validity for Visualization --- \n",
    "can_visualize = False\n",
    "if 'knowledge_graph' not in locals() or not isinstance(knowledge_graph, nx.Graph):\n",
    "    print(\"Error: 'knowledge_graph' not found or is not a NetworkX graph.\")\n",
    "elif knowledge_graph.number_of_nodes() == 0:\n",
    "    print(\"NetworkX Graph is empty. Cannot visualize.\")\n",
    "else:\n",
    "    print(f\"Graph seems valid for visualization ({knowledge_graph.number_of_nodes()} nodes, {knowledge_graph.number_of_edges()} edges).\")\n",
    "    can_visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5bc17734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting nodes...\n",
      "Converted 31 nodes.\n",
      "Converting edges...\n",
      "Converted 21 edges.\n",
      "\n",
      "--- Sample Cytoscape Node Data (First 2) ---\n",
      "[\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"the battle of \\u00e7anakkale\",\n",
      "      \"label\": \"the\\nbattle\\nof\\n\\u00e7anakkale\",\n",
      "      \"degree\": 1,\n",
      "      \"size\": 25.0,\n",
      "      \"tooltip_text\": \"Entity: the battle of \\u00e7anakkale\\nDegree: 1\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"from february 19, 1915, to january 9, 1916\",\n",
      "      \"label\": \"from\\nfebruary\\n19,\\n1915,\\nto\\njanuary\\n9,\\n1916\",\n",
      "      \"degree\": 1,\n",
      "      \"size\": 25.0,\n",
      "      \"tooltip_text\": \"Entity: from february 19, 1915, to january 9, 1916\\nDegree: 1\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Sample Cytoscape Edge Data (First 2) ---\n",
      "[\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"edge_0\",\n",
      "      \"source\": \"the battle of \\u00e7anakkale\",\n",
      "      \"target\": \"from february 19, 1915, to january 9, 1916\",\n",
      "      \"label\": \"took place\",\n",
      "      \"tooltip_text\": \"Relationship: took place\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"data\": {\n",
      "      \"id\": \"edge_1\",\n",
      "      \"source\": \"the allied powers\",\n",
      "      \"target\": \"the dardanelles strait\",\n",
      "      \"label\": \"attempted to force a passage through\",\n",
      "      \"tooltip_text\": \"Relationship: attempted to force a passage through\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "cytoscape_nodes = []\n",
    "cytoscape_edges = []\n",
    "\n",
    "if can_visualize:\n",
    "    print(\"Converting nodes...\")\n",
    "    # Calculate degrees for node sizing\n",
    "    node_degrees = dict(knowledge_graph.degree())\n",
    "    max_degree = max(node_degrees.values()) if node_degrees else 1\n",
    "    \n",
    "    for node_id in knowledge_graph.nodes():\n",
    "        degree = node_degrees.get(node_id, 0)\n",
    "        # Simple scaling for node size (adjust logic as needed)\n",
    "        node_size = 15 + (degree / max_degree) * 50 if max_degree > 0 else 15\n",
    "        \n",
    "        cytoscape_nodes.append({\n",
    "            'data': {\n",
    "                'id': str(node_id), # ID must be string\n",
    "                'label': str(node_id).replace(' ', '\\n'), # Display label (wrap spaces)\n",
    "                'degree': degree,\n",
    "                'size': node_size,\n",
    "                'tooltip_text': f\"Entity: {str(node_id)}\\nDegree: {degree}\" # Tooltip on hover\n",
    "            }\n",
    "        })\n",
    "    print(f\"Converted {len(cytoscape_nodes)} nodes.\")\n",
    "    \n",
    "    print(\"Converting edges...\")\n",
    "    edge_count = 0\n",
    "    for u, v, data in knowledge_graph.edges(data=True):\n",
    "        edge_id = f\"edge_{edge_count}\" # Unique edge ID\n",
    "        predicate_label = data.get('label', '')\n",
    "        cytoscape_edges.append({\n",
    "            'data': {\n",
    "                'id': edge_id,\n",
    "                'source': str(u),\n",
    "                'target': str(v),\n",
    "                'label': predicate_label, # Label on edge\n",
    "                'tooltip_text': f\"Relationship: {predicate_label}\" # Tooltip on hover\n",
    "            }\n",
    "        })\n",
    "        edge_count += 1\n",
    "    print(f\"Converted {len(cytoscape_edges)} edges.\")\n",
    "    \n",
    "    # Combine into the final structure\n",
    "    cytoscape_graph_data = {'nodes': cytoscape_nodes, 'edges': cytoscape_edges}\n",
    "    \n",
    "    # Visualize the converted structure (first few nodes/edges)\n",
    "    print(\"\\n--- Sample Cytoscape Node Data (First 2) ---\")\n",
    "    print(json.dumps(cytoscape_graph_data['nodes'][:2], indent=2))\n",
    "    print(\"\\n--- Sample Cytoscape Edge Data (First 2) ---\")\n",
    "    print(json.dumps(cytoscape_graph_data['edges'][:2], indent=2))\n",
    "    print(\"-\" * 25)\n",
    "else:\n",
    "     print(\"Skipping data conversion as graph is not valid for visualization.\")\n",
    "     cytoscape_graph_data = {'nodes': [], 'edges': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c99197bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ipycytoscape widget...\n",
      "Widget created.\n",
      "Loading graph data into widget...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "if can_visualize:\n",
    "    print(\"Creating ipycytoscape widget...\")\n",
    "    cyto_widget = ipycytoscape.CytoscapeWidget()\n",
    "    print(\"Widget created.\")\n",
    "    \n",
    "    print(\"Loading graph data into widget...\")\n",
    "    cyto_widget.graph.add_graph_from_json(cytoscape_graph_data, directed=True)\n",
    "    print(\"Data loaded.\")\n",
    "else:\n",
    "    print(\"Skipping widget creation.\")\n",
    "    cyto_widget = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0b83c9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining enhanced colorful and interactive visual style...\n",
      "Setting enhanced visual style on widget...\n",
      "Enhanced colorful and interactive style applied successfully.\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Defining enhanced colorful and interactive visual style...\")\n",
    "    # More vibrant and colorful styling with a modern color scheme\n",
    "    visual_style = [\n",
    "        {\n",
    "            'selector': 'node',\n",
    "            'style': {\n",
    "                'label': 'data(label)',\n",
    "                'width': 'data(size)',\n",
    "                'height': 'data(size)',\n",
    "                'background-color': '#3498db',  # Bright blue\n",
    "                'background-opacity': 0.9,\n",
    "                'color': '#ffffff',             # White text\n",
    "                'font-size': '12px',\n",
    "                'font-weight': 'bold',\n",
    "                'text-valign': 'center',\n",
    "                'text-halign': 'center',\n",
    "                'text-wrap': 'wrap',\n",
    "                'text-max-width': '100px',\n",
    "                'text-outline-width': 2,\n",
    "                'text-outline-color': '#2980b9',  # Matching outline\n",
    "                'text-outline-opacity': 0.7,\n",
    "                'border-width': 3,\n",
    "                'border-color': '#1abc9c',      # Turquoise border\n",
    "                'border-opacity': 0.9,\n",
    "                'shape': 'ellipse',\n",
    "                'transition-property': 'background-color, border-color, border-width, width, height',\n",
    "                'transition-duration': '0.3s',\n",
    "                'tooltip-text': 'data(tooltip_text)'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'node:selected',\n",
    "            'style': {\n",
    "                'background-color': '#e74c3c',  # Pomegranate red\n",
    "                'border-width': 4,\n",
    "                'border-color': '#c0392b',\n",
    "                'text-outline-color': '#e74c3c',\n",
    "                'width': 'data(size) * 1.2',    # Enlarge selected nodes\n",
    "                'height': 'data(size) * 1.2'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'node:hover',\n",
    "            'style': {\n",
    "                'background-color': '#9b59b6',  # Purple on hover\n",
    "                'border-width': 4,\n",
    "                'border-color': '#8e44ad',\n",
    "                'cursor': 'pointer',\n",
    "                'z-index': 999\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge',\n",
    "            'style': {\n",
    "                'label': 'data(label)',\n",
    "                'width': 2.5,\n",
    "                'curve-style': 'bezier',\n",
    "                'line-color': '#2ecc71',         # Green\n",
    "                'line-opacity': 0.8,\n",
    "                'target-arrow-color': '#27ae60',\n",
    "                'target-arrow-shape': 'triangle',\n",
    "                'arrow-scale': 1.5,\n",
    "                'font-size': '10px',\n",
    "                'font-weight': 'normal',\n",
    "                'color': '#2c3e50',\n",
    "                'text-background-opacity': 0.9,\n",
    "                'text-background-color': '#ecf0f1',\n",
    "                'text-background-shape': 'roundrectangle',\n",
    "                'text-background-padding': '3px',\n",
    "                'text-rotation': 'autorotate',\n",
    "                'edge-text-rotation': 'autorotate',\n",
    "                'transition-property': 'line-color, width, target-arrow-color',\n",
    "                'transition-duration': '0.3s',\n",
    "                'tooltip-text': 'data(tooltip_text)'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge:selected',\n",
    "            'style': {\n",
    "                'line-color': '#f39c12',         # Yellow-orange\n",
    "                'target-arrow-color': '#d35400',\n",
    "                'width': 4,\n",
    "                'text-background-color': '#f1c40f',\n",
    "                'color': '#ffffff',               # White text\n",
    "                'z-index': 998\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': 'edge:hover',\n",
    "            'style': {\n",
    "                'line-color': '#e67e22',         # Orange on hover\n",
    "                'width': 3.5,\n",
    "                'cursor': 'pointer',\n",
    "                'target-arrow-color': '#d35400',\n",
    "                'z-index': 997\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'selector': '.center-node',\n",
    "            'style': {\n",
    "                'background-color': '#16a085',    # Teal\n",
    "                'background-opacity': 1,\n",
    "                'border-width': 4,\n",
    "                'border-color': '#1abc9c',        # Turquoise border\n",
    "                'border-opacity': 1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Setting enhanced visual style on widget...\")\n",
    "    cyto_widget.set_style(visual_style)\n",
    "    \n",
    "    # Apply a better animated layout\n",
    "    cyto_widget.set_layout(name='cose', \n",
    "                          nodeRepulsion=5000, \n",
    "                          nodeOverlap=40, \n",
    "                          idealEdgeLength=120, \n",
    "                          edgeElasticity=200, \n",
    "                          nestingFactor=6, \n",
    "                          gravity=90, \n",
    "                          numIter=2500,\n",
    "                          animate=True,\n",
    "                          animationDuration=1000,\n",
    "                          initialTemp=300,\n",
    "                          coolingFactor=0.95)\n",
    "    \n",
    "    # Add a special class to main nodes (Marie Curie)\n",
    "    if len(cyto_widget.graph.nodes) > 0:\n",
    "        main_nodes = [node.data['id'] for node in cyto_widget.graph.nodes \n",
    "                     if node.data.get('degree', 0) > 10]\n",
    "        \n",
    "        # Create gradient styles for center nodes\n",
    "        for i, node_id in enumerate(main_nodes):\n",
    "            # Use vibrant colors for center nodes\n",
    "            center_style = {\n",
    "                'selector': f'node[id = \"{node_id}\"]',\n",
    "                'style': {\n",
    "                    'background-color': '#9b59b6',   # Purple\n",
    "                    'background-opacity': 0.95,\n",
    "                    'border-width': 4,\n",
    "                    'border-color': '#8e44ad',      # Darker purple border\n",
    "                    'border-opacity': 1,\n",
    "                    'text-outline-width': 3,\n",
    "                    'text-outline-color': '#8e44ad',\n",
    "                    'font-size': '14px'\n",
    "                }\n",
    "            }\n",
    "            visual_style.append(center_style)\n",
    "        \n",
    "        # Update the style with the new additions\n",
    "        cyto_widget.set_style(visual_style)\n",
    "    \n",
    "    print(\"Enhanced colorful and interactive style applied successfully.\")\n",
    "else:\n",
    "    print(\"Skipping style definition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "805a5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting layout algorithm ('cose')...\n",
      "Layout set. The graph will arrange itself when displayed.\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Setting layout algorithm ('cose')...\")\n",
    "    # cose (Compound Spring Embedder) is often good for exploring connections\n",
    "    cyto_widget.set_layout(name='cose', \n",
    "                           animate=True, \n",
    "                           # Adjust parameters for better spacing/layout\n",
    "                           nodeRepulsion=4000, # Increase repulsion \n",
    "                           nodeOverlap=40,    # Increase overlap avoidance\n",
    "                           idealEdgeLength=120, # Slightly longer ideal edges\n",
    "                           edgeElasticity=150, \n",
    "                           nestingFactor=5, \n",
    "                           gravity=100,        # Increase gravity slightly\n",
    "                           numIter=1500,      # More iterations\n",
    "                           initialTemp=200,\n",
    "                           coolingFactor=0.95,\n",
    "                           minTemp=1.0)\n",
    "    print(\"Layout set. The graph will arrange itself when displayed.\")\n",
    "else:\n",
    "     print(\"Skipping layout setting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f565cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying interactive graph widget below...\n",
      "Interact: Zoom (scroll), Pan (drag background), Move Nodes (drag nodes), Hover for details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270d793fa3174acba159cebaebe7a6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cose', 'nodeRepulsion': 4000, 'nodeOverlap': 40, 'idealEdgeLength':‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "End of Visualization Step.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "if cyto_widget:\n",
    "    print(\"Displaying interactive graph widget below...\")\n",
    "    print(\"Interact: Zoom (scroll), Pan (drag background), Move Nodes (drag nodes), Hover for details.\")\n",
    "    display(cyto_widget)\n",
    "else:\n",
    "    print(\"No widget to display.\")\n",
    "\n",
    "# Add a clear separator\n",
    "print(\"\\n\" + \"-\" * 25 + \"\\nEnd of Visualization Step.\" + \"\\n\" + \"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401be3b-fb8e-4f6a-ad26-9695427452fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
